<!DOCTYPE html>
<html lang="en-us"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="content-type" content="text/html">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title itemprop="name">Qwen 3 ASR vs Voxtral Mini Realtime | M&#39;Sync</title>
<meta property="og:title" content="Qwen 3 ASR vs Voxtral Mini Realtime | M&#39;Sync" />
<meta name="twitter:title" content="Qwen 3 ASR vs Voxtral Mini Realtime | M&#39;Sync" />
<meta itemprop="name" content="Qwen 3 ASR vs Voxtral Mini Realtime | M&#39;Sync" />
<meta name="application-name" content="Qwen 3 ASR vs Voxtral Mini Realtime | M&#39;Sync" />
<meta property="og:site_name" content="M&#39;Sync" />

<meta name="description" content="Notes, experiments, and experience reports on systems, machine learning, and engineering craft.">
<meta itemprop="description" content="Notes, experiments, and experience reports on systems, machine learning, and engineering craft." />
<meta property="og:description" content="Notes, experiments, and experience reports on systems, machine learning, and engineering craft." />
<meta name="twitter:description" content="Notes, experiments, and experience reports on systems, machine learning, and engineering craft." />

<meta property="og:locale" content="en-us" />
<meta name="language" content="en-us" />

  <link rel="alternate" hreflang="en" href="http://localhost:1313/blog/2026/2026-02-15-qwen-3-asr-vs-voxtral-mini-realtime/" title="" />






<meta name="generator" content="Hugo 0.155.3">

    
    <meta property="og:url" content="http://localhost:1313/blog/2026/2026-02-15-qwen-3-asr-vs-voxtral-mini-realtime/">
  <meta property="og:site_name" content="M&#39;Sync">
  <meta property="og:title" content="Qwen 3 ASR vs Voxtral Mini Realtime">
  <meta property="og:description" content="Two Ways a Machine Can Listen: Qwen3-ASR vs Voxtral Realtime A detailed, source-checked architectural comparison of two speech-to-text systems — one centered on segment-style decoding, one designed for native realtime decoding.
1. Why This Comparison Matters This article compares two open ASR systems that target similar use cases but are architecturally very different:
Qwen3-ASR-0.6B (Qwen) Voxtral-Mini-4B-Realtime-2602 (Mistral) Both convert speech to text. Both target practical deployment. But their core inference loops and runtime assumptions are not the same.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2026-02-15T09:00:00+05:30">
    <meta property="article:modified_time" content="2026-02-15T09:00:00+05:30">


    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Qwen 3 ASR vs Voxtral Mini Realtime">
  <meta name="twitter:description" content="Two Ways a Machine Can Listen: Qwen3-ASR vs Voxtral Realtime A detailed, source-checked architectural comparison of two speech-to-text systems — one centered on segment-style decoding, one designed for native realtime decoding.
1. Why This Comparison Matters This article compares two open ASR systems that target similar use cases but are architecturally very different:
Qwen3-ASR-0.6B (Qwen) Voxtral-Mini-4B-Realtime-2602 (Mistral) Both convert speech to text. Both target practical deployment. But their core inference loops and runtime assumptions are not the same.">


    

    <link rel="canonical" href="http://localhost:1313/blog/2026/2026-02-15-qwen-3-asr-vs-voxtral-mini-realtime/">
    <link href="/style.min.c2b918d0b645b6732a537299abbcdb9450c4426034e60ae83fa3b194686e2200.css" rel="stylesheet">
    <link href="/code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css" rel="stylesheet">

    
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg">
    <link rel="shortcut icon" href="/favicon.ico">




<link rel="manifest" href="http://localhost:1313/site.webmanifest">

<meta name="msapplication-config" content="/browserconfig.xml">
<meta name="msapplication-TileColor" content="#2d89ef">
<meta name="theme-color" content="#434648">

    
    <link rel="icon" type="image/svg+xml" href="/icons/favicon.svg">

    
    
      


<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">


<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>


<script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: false });
  document.addEventListener('DOMContentLoaded', () => {
    document.querySelectorAll('code.language-mermaid').forEach((el) => {
      const pre = el.parentElement;
      const div = document.createElement('div');
      div.classList.add('mermaid');
      div.textContent = el.textContent;
      pre.parentElement.replaceChild(div, pre);
    });
    mermaid.run();
  });
</script>
    
</head>
<body data-theme = "light" class="notransition">

<script src="/js/theme.js"></script>

<div class="navbar" role="navigation">
  <nav class="menu" aria-label="Primary navigation">
    <a href="http://localhost:1313/" class="logo" aria-label="M&#39;Sync">
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" 
viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" 
stroke-linejoin="round" class="feather feather-home">
<title>Home</title>
<path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
<polyline points="9 22 9 12 15 12 15 22"></polyline>
</svg>
      <span class="logo-text">M&#39;Sync</span>
    </a>
    <input type="checkbox" id="menu-trigger" class="menu-trigger" aria-hidden="true" />
    <label for="menu-trigger" class="menu-toggle" aria-controls="site-menu" aria-label="Toggle navigation">
      <span class="menu-icon">
        <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" stroke="currentColor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7L3.40726 7"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488L3.49301 3.51488"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"></path><path stroke-linecap="round" stroke-linejoin="round" d="M0.5 12.5V1.5C0.5 0.947715 0.947715 0.5 1.5 0.5H12.5C13.0523 0.5 13.5 0.947715 13.5 1.5V12.5C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C0.947715 13.5 0.5 13.0523 0.5 12.5Z"></path></svg>
      </span>
    </label>

    <div class="trigger" id="site-menu">
      <ul class="trigger-container">
        
        
        <li>
          <a class="menu-link " href="/">
            Home
          </a>
          
        </li>
        
        <li>
          <a class="menu-link " href="/notes/">
            Notes
          </a>
          
        </li>
        
        <li>
          <a class="menu-link " href="/blog/">
            Blog
          </a>
          
        </li>
        
        
      </ul>
      <a id="mode" href="#" aria-label="Toggle color scheme">
        <svg xmlns="http://www.w3.org/2000/svg" class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>Light mode</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
        <svg xmlns="http://www.w3.org/2000/svg" class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>Dark mode</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
      </a>
    </div>
  </nav>
</div>

<div class="wrapper post">
    <main class="page-content" aria-label="Content">
        <article>
            <header class="header">
                <h1 class="header-title">Qwen 3 ASR vs Voxtral Mini Realtime</h1>
                
                
                
                <div class="post-meta">
                    <time datetime="2026-02-15T09:00:00&#43;05:30" itemprop="datePublished"> Feb 15, 2026 </time>
                </div>
                
            </header>
            
            <div class="page-content">
                <h1 id="two-ways-a-machine-can-listen-qwen3-asr-vs-voxtral-realtime">Two Ways a Machine Can Listen: Qwen3-ASR vs Voxtral Realtime</h1>
<p><em>A detailed, source-checked architectural comparison of two speech-to-text systems — one centered on segment-style decoding, one designed for native realtime decoding.</em></p>
<hr>
<h2 id="1-why-this-comparison-matters">1. Why This Comparison Matters</h2>
<p>This article compares two open ASR systems that target similar use cases but are architecturally very different:</p>
<ul>
<li><strong>Qwen3-ASR-0.6B</strong> (Qwen)</li>
<li><strong>Voxtral-Mini-4B-Realtime-2602</strong> (Mistral)</li>
</ul>
<p>Both convert speech to text. Both target practical deployment. But their core inference loops and runtime assumptions are not the same.</p>
<p>That difference matters for engineering decisions in realtime products:</p>
<ul>
<li>how to design backend interfaces,</li>
<li>how to segment and emit partial/final text,</li>
<li>where to place latency controls,</li>
<li>and how much of a stack should be model-agnostic.</li>
</ul>
<p>The focus is architecture and system behavior, not benchmark ranking. Checkpoint-specific numeric values are anchored to released configuration files (<code>config.json</code>, <code>preprocessor_config.json</code>, <code>params.json</code>), with explicit notes where values come from implementation code, paper-reported totals, or runtime behavior.</p>
<hr>
<h2 id="2-background-common-stt-architecture-families">2. Background: Common STT Architecture Families</h2>
<p>There is no canonical finite list of STT architectures. Most production systems are variants, combinations, or evolutions of a small set of recurring families.</p>
<h3 id="21-ctc-connectionist-temporal-classification">2.1 CTC (Connectionist Temporal Classification)</h3>
<ul>
<li>Typical shape: encoder-only acoustic model, optional external LM for rescoring.</li>
<li>Intuition: align frame-level audio evidence with token outputs under a monotonic constraint.</li>
<li>Practical profile: efficient and robust; widely used in offline and near-realtime systems.</li>
</ul>
<h3 id="22-rnn-t--transducer">2.2 RNN-T / Transducer</h3>
<ul>
<li>Typical shape: encoder + prediction network + joiner.</li>
<li>Intuition: produce tokens incrementally while maintaining monotonic alignment.</li>
<li>Practical profile: a dominant choice for streaming ASR in many production deployments.</li>
</ul>
<h3 id="23-attention-encoder-decoder-seq2seq">2.3 Attention Encoder-Decoder (Seq2Seq)</h3>
<ul>
<li>Typical shape: encoder builds acoustic representation; decoder autoregressively generates text.</li>
<li>Intuition: learn richer sequence mapping with flexible conditioning.</li>
<li>Practical profile: strong quality in offline settings; streaming variants exist but require additional design constraints.</li>
</ul>
<h3 id="24-two-pass-hybrids">2.4 Two-Pass Hybrids</h3>
<ul>
<li>Typical shape: fast first pass (often streaming) + slower second pass rescoring/correction.</li>
<li>Intuition: trade latency and quality by splitting tasks across two inference stages.</li>
</ul>
<h3 id="25-speech-llm-hybrids">2.5 Speech-LLM Hybrids</h3>
<ul>
<li>Typical shape: speech/audio encoder feeding a language-model decoder.</li>
<li>Intuition: leverage LM capabilities (prompting, formatting control, multilingual generalization) in ASR.</li>
</ul>
<p>Qwen3-ASR and Voxtral Realtime both fall into this last category — they fuse an audio encoder with a language model decoder — but they do so in fundamentally different ways.</p>
<p><strong>Key takeaway:</strong> a useful cross-model abstraction should target <em>behavioral contracts</em> (session lifecycle, events, capabilities), not force identical internals.</p>
<hr>
<h2 id="3-shared-signal-path">3. Shared Signal Path</h2>
<p>Both models follow the same high-level pipeline from waveform to text:</p>
<pre tabindex="0"><code class="language-mermaid" data-lang="mermaid">flowchart LR
    A[&#34;Speech waveform&#34;] --&gt; B[&#34;Microphone / ADC (16 kHz mono)&#34;]
    B --&gt; C[&#34;Feature extraction (mel frames)&#34;]
    C --&gt; D[&#34;Neural audio-text model&#34;]
    D --&gt; E[&#34;Token stream&#34;]
    E --&gt; F[&#34;Transcript text&#34;]
</code></pre><p>The major differences are in:</p>
<ul>
<li>how audio features are encoded,</li>
<li>how audio and text streams are fused,</li>
<li>and how decoding steps are scheduled over time.</li>
</ul>
<hr>
<h2 id="4-building-blocks-a-brief-primer">4. Building Blocks (a brief primer)</h2>
<h3 id="41-mel-spectrograms-what-the-model-actually-sees">4.1 Mel Spectrograms: What the Model Actually &ldquo;Sees&rdquo;</h3>
<p>Raw audio at 16 kHz means 16,000 samples per second. Most ASR models operate on short-time spectral features instead of raw waveform samples.</p>
<p>The audio is sliced into overlapping windows (typically 25ms wide, sliding every 10ms), and for each window, the energy at different frequency bands is measured. The &ldquo;mel&rdquo; part refers to a perceptual scale — it spaces frequency bands the way human hearing works, with finer resolution at low frequencies.</p>
<pre tabindex="0"><code>                    Time →
            ┌──────────────────────────┐
  High freq │░░▓▓░░░░░░▓▓▓▓░░░░░░░░░░░│
            │░▓▓▓░░░░░▓▓▓▓▓▓░░░░░░░░░░│
            │▓▓▓▓▓░░░▓▓▓▓▓▓▓▓░░░░░░░░░│
            │▓▓▓▓▓▓░▓▓▓▓▓▓▓▓▓▓░░░░░░░░│
  Low freq  │▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓░░▓▓░░░│
            └──────────────────────────┘
              &#34;Hello&#34;       &#34;world&#34;  (silence)
</code></pre><p>Both checkpoints use <strong>128 mel bins</strong> and <strong>hop length 160</strong> (10ms at 16 kHz), so both produce ~100 mel frames per second.</p>
<p>If duration is <code>D</code> seconds:</p>
<ul>
<li><code>T ≈ 100 × D</code></li>
<li>mel features: <code>X ∈ ℝ^(128 × T)</code></li>
</ul>
<h3 id="42-embeddings-and-dimensional-alignment">4.2 Embeddings and Dimensional Alignment</h3>
<p>Transformers operate on <strong>embeddings</strong> — fixed-length vectors (typically 1024 to 3072 numbers long) that represent meaning in a geometric space. Things that are similar end up close together: the embedding for &ldquo;dog&rdquo; is near &ldquo;puppy&rdquo; and far from &ldquo;algebra.&rdquo;</p>
<p>For multimodal ASR, audio representations must be mapped into the decoder&rsquo;s hidden space (or a compatible fusion space). The details differ:</p>
<ul>
<li>Qwen3-ASR: audio embeddings <strong>replace</strong> placeholder tokens in a text prompt sequence.</li>
<li>Voxtral Realtime: audio and text embeddings are <strong>added together</strong> at each timestep.</li>
</ul>
<p>Both approaches require audio and text to share the same vector space and dimensionality.</p>
<h3 id="43-bidirectional-vs-causal-attention">4.3 Bidirectional vs Causal Attention</h3>
<p>A <strong>transformer</strong> processes a sequence of embeddings. Its core mechanism is <strong>attention</strong>: each position can &ldquo;look at&rdquo; other positions to gather context.</p>
<pre tabindex="0"><code>  Bidirectional                 Causal
  (Qwen3-ASR encoder)          (Voxtral encoder)

  Position: 1 2 3 4 5          Position: 1 2 3 4 5
       1    ✓ ✓ ✓ ✓ ✓               1    ✓ · · · ·
       2    ✓ ✓ ✓ ✓ ✓               2    ✓ ✓ · · ·
       3    ✓ ✓ ✓ ✓ ✓               3    ✓ ✓ ✓ · ·
       4    ✓ ✓ ✓ ✓ ✓               4    ✓ ✓ ✓ ✓ ·
       5    ✓ ✓ ✓ ✓ ✓               5    ✓ ✓ ✓ ✓ ✓

  ✓ = can attend    · = cannot attend
</code></pre><ul>
<li><strong>Bidirectional</strong>: every position sees every other. Richer representations, but requires the complete input.</li>
<li><strong>Causal</strong>: each position only sees past/present. Required for strict realtime — future frames haven&rsquo;t arrived yet.</li>
</ul>
<h3 id="44-autoregressive-decoding-and-kv-cache">4.4 Autoregressive Decoding and KV Cache</h3>
<p>Both models generate text <strong>autoregressively</strong>: they predict one token at a time, and each prediction depends on all previous tokens.</p>
<pre tabindex="0"><code>Step 1: [audio context]                   → predict &#34;The&#34;
Step 2: [audio context] + &#34;The&#34;           → predict &#34; revenue&#34;
Step 3: [audio context] + &#34;The revenue&#34;   → predict &#34; increased&#34;
...
</code></pre><p>The decoder maintains a <strong>KV cache</strong> — a running memory of previously computed attention keys and values — so each new token doesn&rsquo;t reprocess the full history.</p>
<hr>
<h2 id="5-qwen3-asr-06b-architecture-and-runtime-behavior">5. Qwen3-ASR-0.6B: Architecture and Runtime Behavior</h2>
<h3 id="51-source-checked-configuration">5.1 Source-Checked Configuration</h3>
<p>All values from <a href="https://huggingface.co/Qwen/Qwen3-ASR-0.6B/blob/main/config.json"><code>config.json</code></a> and <a href="https://huggingface.co/Qwen/Qwen3-ASR-0.6B/blob/main/preprocessor_config.json"><code>preprocessor_config.json</code></a>:</p>
<p>If you&rsquo;re skimming: this checkpoint is relatively compact (<code>18</code>-layer audio encoder + <code>28</code>-layer decoder at hidden size <code>1024</code>).</p>
<table>
  <thead>
      <tr>
          <th>Component</th>
          <th>Value</th>
          <th>Source field</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Sample rate</td>
          <td>16,000 Hz</td>
          <td><code>preprocessor_config.json</code></td>
      </tr>
      <tr>
          <td>Mel bins</td>
          <td>128</td>
          <td><code>feature_size: 128</code></td>
      </tr>
      <tr>
          <td><code>n_fft</code> / <code>hop_length</code></td>
          <td>400 / 160</td>
          <td><code>preprocessor_config.json</code></td>
      </tr>
      <tr>
          <td>Conv downsampling hidden</td>
          <td>480</td>
          <td><code>downsample_hidden_size</code></td>
      </tr>
      <tr>
          <td>Conv strides</td>
          <td><code>[2, 2, 2]</code> (8× total)</td>
          <td>3 Conv2D layers</td>
      </tr>
      <tr>
          <td>Audio encoder layers</td>
          <td>18</td>
          <td><code>encoder_layers</code></td>
      </tr>
      <tr>
          <td>Audio encoder dim</td>
          <td>896</td>
          <td><code>d_model</code></td>
      </tr>
      <tr>
          <td>Audio encoder heads</td>
          <td>14</td>
          <td><code>encoder_attention_heads</code></td>
      </tr>
      <tr>
          <td>Audio encoder FFN</td>
          <td>3584</td>
          <td><code>encoder_ffn_dim</code></td>
      </tr>
      <tr>
          <td>Audio output projection</td>
          <td>1024</td>
          <td><code>output_dim</code></td>
      </tr>
      <tr>
          <td>Chunk / window</td>
          <td>n_window=50, n_window_infer=800</td>
          <td><code>audio_config</code></td>
      </tr>
      <tr>
          <td>Decoder layers</td>
          <td>28</td>
          <td><code>num_hidden_layers</code></td>
      </tr>
      <tr>
          <td>Decoder hidden</td>
          <td>1024</td>
          <td><code>hidden_size</code></td>
      </tr>
      <tr>
          <td>Decoder query heads</td>
          <td>16</td>
          <td><code>num_attention_heads</code></td>
      </tr>
      <tr>
          <td>Decoder KV heads</td>
          <td>8</td>
          <td><code>num_key_value_heads</code></td>
      </tr>
      <tr>
          <td>Decoder head dim</td>
          <td>128</td>
          <td><code>head_dim</code></td>
      </tr>
      <tr>
          <td>Decoder FFN</td>
          <td>3072</td>
          <td><code>intermediate_size</code></td>
      </tr>
      <tr>
          <td>RoPE theta</td>
          <td>1,000,000</td>
          <td><code>rope_theta</code></td>
      </tr>
      <tr>
          <td>Vocabulary</td>
          <td>151,936</td>
          <td><code>vocab_size</code></td>
      </tr>
      <tr>
          <td>Tied embeddings</td>
          <td>yes</td>
          <td><code>tie_word_embeddings: true</code></td>
      </tr>
      <tr>
          <td>Audio token ID</td>
          <td>151,676</td>
          <td><code>audio_token_id</code></td>
      </tr>
  </tbody>
</table>
<h3 id="52-end-to-end-flow">5.2 End-to-End Flow</h3>
<pre tabindex="0"><code class="language-mermaid" data-lang="mermaid">flowchart LR
    A[&#34;Audio (16 kHz)&#34;] --&gt; B[&#34;Whisper-style mel (128 × T)&#34;]
    B --&gt; C[&#34;3× Conv2D downsampling (8× in time)&#34;]
    C --&gt; D[&#34;Audio encoder (18L, d=896, bidirectional/chunked)&#34;]
    D --&gt; E[&#34;Project 896 → 1024&#34;]
    E --&gt; F[&#34;Replace audio-pad slots in prompt&#34;]
    F --&gt; G[&#34;Qwen decoder (28L, d=1024, GQA 16q/8kv)&#34;]
    G --&gt; H[&#34;Autoregressive token output&#34;]
</code></pre><h3 id="53-step-by-step">5.3 Step-by-Step</h3>
<h4 id="feature-extraction">Feature Extraction</h4>
<p>The audio signal is converted into a mel spectrogram using the Whisper feature extractor (<code>n_fft=400</code>, <code>hop_length=160</code>):</p>
<pre tabindex="0"><code>mel ∈ ℝ^(128 × T)     where T ≈ 100 × duration_seconds
</code></pre><p>For a 5-second clip: <code>mel</code> has shape <code>[128, 500]</code>.</p>
<h4 id="convolutional-downsampling">Convolutional Downsampling</h4>
<p>Three 2D convolutions with stride 2 shrink both time and frequency:</p>
<pre tabindex="0"><code>Input:  [128 freq × T time × 1 channel]
Conv1:  stride 2, GELU → [64 × T/2 × 480]
Conv2:  stride 2, GELU → [32 × T/4 × 480]
Conv3:  stride 2, GELU → [16 × T/8 × 480]
Flatten + project:      → [T/8, 896]
</code></pre><p>After 8× time compression: roughly <strong>12.5 frames per second</strong>, one embedding per ~80ms.</p>
<h4 id="transformer-encoder-bidirectional-chunked">Transformer Encoder (Bidirectional, Chunked)</h4>
<p>18 transformer layers, each with:</p>
<ul>
<li><strong>Bidirectional self-attention</strong>: every frame can see every other frame</li>
<li><strong>Feed-forward</strong>: GELU, 896 → 3584 → 896</li>
<li><strong>LayerNorm</strong>, residual connections</li>
</ul>
<p>Symbolically, each block:</p>
<pre tabindex="0"><code>h&#39;_l = h_l + MHA(LayerNorm(h_l))
h_{l+1} = h&#39;_l + FFN(LayerNorm(h&#39;_l))
</code></pre><p>For long audio, attention is restricted to within chunks using a block-diagonal mask:</p>
<pre tabindex="0"><code>Audio frames:     [  chunk 1   |  chunk 2   |  chunk 3   ]

Attention mask:   ┌───┬───┬───┐
                  │ ✓ │   │   │  ← chunk 1 attends only to chunk 1
                  ├───┼───┼───┤
                  │   │ ✓ │   │  ← chunk 2 attends only to chunk 2
                  ├───┼───┼───┤
                  │   │   │ ✓ │  ← chunk 3 attends only to chunk 3
                  └───┴───┴───┘
</code></pre><pre tabindex="0"><code class="language-mermaid" data-lang="mermaid">flowchart TB
    subgraph C1[&#34;Chunk 1&#34;]
    A1[&#34;f1&#34;] --- A2[&#34;f2&#34;] --- A3[&#34;f3&#34;]
    end
    subgraph C2[&#34;Chunk 2&#34;]
    B1[&#34;f4&#34;] --- B2[&#34;f5&#34;] --- B3[&#34;f6&#34;]
    end
    subgraph C3[&#34;Chunk 3&#34;]
    D1[&#34;f7&#34;] --- D2[&#34;f8&#34;] --- D3[&#34;f9&#34;]
    end
</code></pre><p>The final output is projected to decoder dimension:</p>
<pre tabindex="0"><code>encoder_out ∈ ℝ^(N × 896) → LN → GELU(Linear) → Linear → ℝ^(N × 1024)
</code></pre><h4 id="placeholder-replacement-how-audio-meets-text">Placeholder Replacement (How Audio Meets Text)</h4>
<p>The model uses a chat-style prompt with audio placeholder tokens:</p>
<pre tabindex="0"><code>&lt;|im_start|&gt;system
{optional context}&lt;|im_end|&gt;
&lt;|im_start|&gt;user
&lt;|audio_start|&gt;&lt;|audio_pad|&gt;×N&lt;|audio_end|&gt;&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
language English&lt;asr_text&gt;
</code></pre><p>The <code>N</code> pad token embeddings are <strong>replaced</strong> with the encoder output embeddings:</p>
<pre tabindex="0"><code>Before replacement:
  [sys_tokens] [user_tokens] [pad₁] [pad₂] ... [padₙ] [asst_tokens]
       ↓            ↓          ↓      ↓           ↓         ↓
    text_emb     text_emb   pad_emb pad_emb ... pad_emb  text_emb

After replacement:
  [sys_tokens] [user_tokens] [pad₁] [pad₂] ... [padₙ] [asst_tokens]
       ↓            ↓          ↓      ↓           ↓         ↓
    text_emb     text_emb   aud₁    aud₂   ... audₙ     text_emb
</code></pre><p>Formally, let <code>E ∈ ℝ^(L × 1024)</code> be prompt embeddings with audio-pad positions <code>p₁...pₙ</code>:</p>
<ul>
<li><code>E'ᵢ = Aⱼ</code> if <code>i = pⱼ</code> (replace with audio embedding)</li>
<li><code>E'ᵢ = Eᵢ</code> otherwise (keep text embedding)</li>
</ul>
<p>The decoder sees a single mixed embedding sequence. It doesn&rsquo;t know which positions carry audio vs. text information.</p>
<h4 id="text-decoding">Text Decoding</h4>
<p>Standard autoregressive generation:</p>
<pre tabindex="0"><code>y_t ~ p(y_t | y_&lt;t, E&#39;)
</code></pre><p>The decoder is a 28-layer Qwen3-style transformer with <strong>Grouped-Query Attention</strong> (GQA): 16 query heads share 8 KV heads (2:1 ratio), halving KV cache size. The feed-forward uses <strong>SwiGLU</strong>:</p>
<pre tabindex="0"><code>FFN(x) = W_down( SiLU(W_gate(x)) ⊙ W_up(x) )
</code></pre><p>In common Qwen3-ASR decoding templates, generation stops on EOS token IDs (for example, 151645 and 151643); exact stop sets are tokenizer/runtime dependent.</p>
<h3 id="54-pipeline-summary-1-second-of-audio">5.4 Pipeline Summary (1 second of audio)</h3>
<pre tabindex="0"><code>16,000 samples
  → mel:         [128, 100]     (128 bins × 100 frames)
  → after convs: [~13, 896]    (12.5 frames/sec)
  → after enc:   [~13, 1024]   (projected to decoder dim)
  → replaces ~13 pad tokens in prompt
  → decoder generates ~5-15 text tokens until EOS
</code></pre><h3 id="55-runtime-implications">5.5 Runtime Implications</h3>
<p>Qwen&rsquo;s official model card reports unified online/offline inference support in their serving stack. In many custom deployments (including ours), integration is segment/batch-style: periodic windows or VAD-bounded turns.</p>
<p>This is important operationally: &ldquo;supports streaming&rdquo; at model/tooling level does not force a particular app-level control loop. The bidirectional encoder fundamentally benefits from seeing full audio, so segment-style integration is a natural fit. In this article, &ldquo;segment-style&rdquo; describes an integration pattern, not a hard model impossibility.</p>
<h3 id="56-generation-loop-shape">5.6 Generation Loop Shape</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Simplified pseudocode for segment-style decoding</span>
</span></span><span class="line"><span class="cl"><span class="n">mel</span> <span class="o">=</span> <span class="n">featurize</span><span class="p">(</span><span class="n">audio_segment</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">audio_embeds</span> <span class="o">=</span> <span class="n">encode_and_project</span><span class="p">(</span><span class="n">mel</span><span class="p">)</span>      <span class="c1"># [N, 1024]</span>
</span></span><span class="line"><span class="cl"><span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">replace_pads</span><span class="p">(</span><span class="n">audio_embeds</span><span class="p">)</span>   <span class="c1"># audio injected into prompt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">cache</span> <span class="o">=</span> <span class="n">init_kv_cache</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_tokens</span><span class="p">):</span>               <span class="c1"># unconstrained loop</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">decoder_step</span><span class="p">(</span><span class="n">prompt_embeds</span><span class="p">,</span> <span class="n">cache</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">token</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">eos_ids</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">break</span>
</span></span><span class="line"><span class="cl">    <span class="k">yield</span> <span class="n">token</span>
</span></span></code></pre></div><p>The loop runs until the model decides to stop. There is no fixed relationship between the number of audio frames and the number of generated tokens.</p>
<hr>
<h2 id="6-voxtral-mini-4b-realtime-2602-architecture-and-runtime-behavior">6. Voxtral-Mini-4B-Realtime-2602: Architecture and Runtime Behavior</h2>
<h3 id="61-source-checked-configuration">6.1 Source-Checked Configuration</h3>
<p>All values from <a href="https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602/blob/main/params.json"><code>params.json</code></a>:</p>
<p>If you&rsquo;re skimming: this checkpoint is larger and explicitly realtime-oriented (causal 32-layer audio encoder + 26-layer 3072-d decoder with delay conditioning).</p>
<table>
  <thead>
      <tr>
          <th>Component</th>
          <th>Value</th>
          <th>Source field</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Sample rate</td>
          <td>16,000 Hz</td>
          <td><code>audio_encoding_args.sampling_rate</code></td>
      </tr>
      <tr>
          <td>Frame rate (output)</td>
          <td>12.5 Hz</td>
          <td><code>audio_encoding_args.frame_rate</code></td>
      </tr>
      <tr>
          <td>Mel bins</td>
          <td>128</td>
          <td><code>audio_encoding_args.num_mel_bins</code></td>
      </tr>
      <tr>
          <td>Window / hop</td>
          <td>window=400, hop=160</td>
          <td><code>audio_encoding_args</code></td>
      </tr>
      <tr>
          <td>Global log mel max</td>
          <td>1.5</td>
          <td><code>audio_encoding_args.global_log_mel_max</code></td>
      </tr>
      <tr>
          <td>Conv stem 1</td>
          <td>causal Conv1D, 128→1280, k=3, s=1</td>
          <td>encoder implementation¹</td>
      </tr>
      <tr>
          <td>Conv stem 2</td>
          <td>causal Conv1D, 1280→1280, k=3, s=2</td>
          <td>encoder implementation¹</td>
      </tr>
      <tr>
          <td>Audio encoder layers</td>
          <td>32</td>
          <td><code>encoder_args.n_layers</code></td>
      </tr>
      <tr>
          <td>Audio encoder dim</td>
          <td>1280</td>
          <td><code>encoder_args.dim</code></td>
      </tr>
      <tr>
          <td>Audio encoder heads</td>
          <td>32</td>
          <td><code>encoder_args.n_heads</code></td>
      </tr>
      <tr>
          <td>Audio encoder KV heads</td>
          <td>32 (full MHA)</td>
          <td><code>encoder_args.n_kv_heads</code></td>
      </tr>
      <tr>
          <td>Audio encoder head dim</td>
          <td>64</td>
          <td><code>encoder_args.head_dim</code></td>
      </tr>
      <tr>
          <td>Audio encoder FFN</td>
          <td>5120</td>
          <td><code>encoder_args.hidden_dim</code></td>
      </tr>
      <tr>
          <td>Audio encoder FFN type</td>
          <td>SwiGLU</td>
          <td><code>encoder_args.ffn_type</code></td>
      </tr>
      <tr>
          <td>Audio encoder norm</td>
          <td>RMSNorm</td>
          <td><code>encoder_args.norm_type</code></td>
      </tr>
      <tr>
          <td>Audio encoder position</td>
          <td>RoPE</td>
          <td><code>encoder_args.pos_embed</code></td>
      </tr>
      <tr>
          <td>Audio encoder attention</td>
          <td>causal</td>
          <td><code>encoder_args.causal: true</code></td>
      </tr>
      <tr>
          <td>Audio encoder window</td>
          <td>750 frames</td>
          <td><code>encoder_args.sliding_window</code></td>
      </tr>
      <tr>
          <td>Encoder biases</td>
          <td>yes</td>
          <td><code>encoder_args.use_biases: true</code></td>
      </tr>
      <tr>
          <td>Downsample factor</td>
          <td>4</td>
          <td><code>downsample_args.downsample_factor</code></td>
      </tr>
      <tr>
          <td>Adapter MLP</td>
          <td>5120→3072→3072</td>
          <td>(1280×4 concat → project)¹</td>
      </tr>
      <tr>
          <td><strong>Decoder layers</strong></td>
          <td><strong>26</strong></td>
          <td><code>n_layers</code></td>
      </tr>
      <tr>
          <td><strong>Decoder dim</strong></td>
          <td><strong>3072</strong></td>
          <td><code>dim</code></td>
      </tr>
      <tr>
          <td><strong>Decoder heads</strong></td>
          <td><strong>32</strong></td>
          <td><code>n_heads</code></td>
      </tr>
      <tr>
          <td><strong>Decoder KV heads</strong></td>
          <td><strong>8</strong></td>
          <td><code>n_kv_heads</code></td>
      </tr>
      <tr>
          <td><strong>Decoder head dim</strong></td>
          <td><strong>128</strong></td>
          <td><code>head_dim</code></td>
      </tr>
      <tr>
          <td><strong>Decoder FFN</strong></td>
          <td><strong>9216</strong></td>
          <td><code>hidden_dim</code></td>
      </tr>
      <tr>
          <td>Decoder biases</td>
          <td>no</td>
          <td><code>use_biases: false</code></td>
      </tr>
      <tr>
          <td><strong>Decoder sliding window</strong></td>
          <td><strong>8192</strong></td>
          <td><code>sliding_window</code></td>
      </tr>
      <tr>
          <td>Max sequence length</td>
          <td>131,072</td>
          <td><code>model_max_length</code></td>
      </tr>
      <tr>
          <td>Vocabulary</td>
          <td>131,072 (Tekken)</td>
          <td><code>vocab_size</code></td>
      </tr>
      <tr>
          <td>Tied embeddings</td>
          <td>yes</td>
          <td><code>tied_embeddings: true</code></td>
      </tr>
      <tr>
          <td>RoPE theta</td>
          <td>1,000,000</td>
          <td><code>rope_theta</code></td>
      </tr>
      <tr>
          <td>Ada RMS-Norm enabled</td>
          <td>yes</td>
          <td><code>ada_rms_norm_t_cond: true</code></td>
      </tr>
      <tr>
          <td>Ada RMS-Norm inner dim</td>
          <td>32</td>
          <td><code>ada_rms_norm_t_cond_dim</code></td>
      </tr>
      <tr>
          <td>Total parameters</td>
          <td>~4.4B</td>
          <td>encoder 970M + adapter 25M + decoder 3.4B</td>
      </tr>
  </tbody>
</table>
<p>¹ Conv stem and adapter MLP architecture details come from implementation code (mlx-audio, vLLM), not from <code>params.json</code> directly. The params file specifies the encoder dimensions and downsample factor; the conv topology is an implementation detail.</p>
<p><code>frame_rate=12.5</code> here refers to the stream-synchronous decoding rate after internal downsampling. Frontend mel extraction is still ~100 Hz, with an intermediate ~50 Hz stage after the conv stem.</p>
<p><strong>Note on <code>n_fft</code>:</strong> <code>params.json</code> does not specify <code>n_fft</code>. The value 512 used in some implementations (e.g., mlx-audio) is likely an implementation default. Only <code>window_size=400</code> and <code>hop_length=160</code> are specified in the primary config.</p>
<h3 id="62-end-to-end-flow">6.2 End-to-End Flow</h3>
<pre tabindex="0"><code class="language-mermaid" data-lang="mermaid">flowchart LR
    A[&#34;Audio (16 kHz)&#34;] --&gt; B[&#34;Mel features (128 × T, 100 Hz)&#34;]
    B --&gt; C[&#34;Causal Conv stem → 50 Hz, d=1280&#34;]
    C --&gt; D[&#34;Causal encoder (32L, d=1280, sliding window=750)&#34;]
    D --&gt; E[&#34;Downsample ×4 + adapter MLP → d=3072, 12.5 Hz&#34;]
    E --&gt; F[&#34;Per-step fusion: audio_t + text_embed(prev token)&#34;]
    F --&gt; G[&#34;LM decoder (26L, d=3072, GQA 32q/8kv, AdaNorm)&#34;]
    G --&gt; H[&#34;Token stream (lexical + padding/boundary control tokens)&#34;]
</code></pre><h3 id="63-step-by-step">6.3 Step-by-Step</h3>
<h4 id="feature-extraction-1">Feature Extraction</h4>
<p>Same 128-bin mel spectrogram as Qwen3-ASR, but with Slaney-normalized filterbanks and a different log normalization:</p>
<pre tabindex="0"><code>log_spec = log10(max(mel, 1e-10))
log_spec = clamp(log_spec, min=global_log_mel_max - 8.0)
log_spec = (log_spec + 4.0) / 4.0
</code></pre><p>where <code>global_log_mel_max = 1.5</code> (from params.json). Not interchangeable with Whisper&rsquo;s feature extractor.</p>
<h4 id="causal-convolutional-stem">Causal Convolutional Stem</h4>
<p>Two 1D convolutions (not 2D like Qwen3) with <strong>causal padding</strong> — each output frame depends only on current and past input frames:</p>
<pre tabindex="0"><code>Input:   [128, T]  (128 mel bins as input channels)
Conv1:   k=3, s=1, causal → [1280, T]     (expand channels, keep time)
Conv2:   k=3, s=2, causal → [1280, T/2]   (halve time)
</code></pre><p>Causal vs. standard convolution (kernel=3):</p>
<pre tabindex="0"><code>Standard:  output[t] depends on input[t-1], input[t], input[t+1]
                                                       ↑ needs future!
Causal:    output[t] depends on input[t-2], input[t-1], input[t]
                                                        ✓ only past/present
</code></pre><p>Output rate: 50 Hz (one embedding every 20ms). Dimension: 1280.</p>
<h4 id="causal-transformer-encoder">Causal Transformer Encoder</h4>
<p>32 transformer layers process the convolution output. Every frame can only attend to past and present frames — never future frames.</p>
<p>The <strong>sliding window</strong> of 750 frames provides ~15 seconds of audio context at 50 Hz. This bounds memory for arbitrarily long audio:</p>
<pre tabindex="0"><code>Sliding window attention (window=5, for illustration):

Frame:     1  2  3  4  5  6  7  8  9  10
   1      [✓  ·  ·  ·  ·  ·  ·  ·  ·  · ]
   2      [✓  ✓  ·  ·  ·  ·  ·  ·  ·  · ]
   3      [✓  ✓  ✓  ·  ·  ·  ·  ·  ·  · ]
   4      [✓  ✓  ✓  ✓  ·  ·  ·  ·  ·  · ]
   5      [✓  ✓  ✓  ✓  ✓  ·  ·  ·  ·  · ]
   6      [·  ✓  ✓  ✓  ✓  ✓  ·  ·  ·  · ]  ← frame 1 falls out of window
   7      [·  ·  ✓  ✓  ✓  ✓  ✓  ·  ·  · ]
   ...
</code></pre><p>Each layer uses:</p>
<ul>
<li><strong>Causal self-attention</strong> with sliding window and RoPE (θ = 10⁶)</li>
<li><strong>SwiGLU FFN</strong>: 1280 → 5120 → 1280</li>
<li><strong>RMSNorm</strong> (pre-norm)</li>
<li><strong>Full MHA</strong> (32 heads, 32 KV heads — no GQA in the encoder)</li>
<li><strong>Biases</strong> on attention projections (unlike the decoder)</li>
</ul>
<h4 id="4-downsampling--adapter-mlp">4× Downsampling + Adapter MLP</h4>
<p>The encoder output (50 Hz, 1280-dim) is too frequent for the decoder. Four consecutive frames are concatenated and projected:</p>
<pre tabindex="0"><code>encoder output:  e₁  e₂  e₃  e₄  e₅  e₆  e₇  e₈  ...   (50 Hz, 1280-dim)
                 └──────┬──────┘  └──────┬──────┘
                   concat(4)         concat(4)
                      ↓                  ↓
                [e₁;e₂;e₃;e₄]    [e₅;e₆;e₇;e₈]          (12.5 Hz, 5120-dim)
                      ↓                  ↓
              GELU(Linear(5120→3072))                       (12.5 Hz, 3072-dim)
                      ↓                  ↓
              Linear(3072→3072)                             (12.5 Hz, 3072-dim)
                      ↓                  ↓
                     a₁                 a₂                  audio embeddings
</code></pre><p>One audio embedding per 80ms, dimension 3072 — matching the decoder hidden size.</p>
<h4 id="additive-fusion-how-audio-meets-text">Additive Fusion (How Audio Meets Text)</h4>
<p>Instead of replacing placeholders, Voxtral <strong>adds</strong> audio and text embeddings at every timestep:</p>
<pre tabindex="0"><code>z_t = A_t + e(y_{t-1})
</code></pre><p>where <code>A_t ∈ ℝ^3072</code> is the adapted audio embedding and <code>e(y_{t-1}) ∈ ℝ^3072</code> is the embedding of the previous token.</p>
<p>A concrete trace of &ldquo;The revenue increased&rdquo; with delay = 6 frames (480ms):</p>
<pre tabindex="0"><code>Time   Audio embed       Previous token    Fused input         Output
─────  ────────────────  ────────────────  ─────────────────   ────────────
 0ms   a₀ (silence)    + embed(BOS)      = z₀                → [P] (padding)
80ms   a₁ (silence)    + embed([P])      = z₁                → [P]
160ms  a₂ (&#34;Th-&#34;)      + embed([P])      = z₂                → [P]
240ms  a₃ (&#34;-e re-&#34;)   + embed([P])      = z₃                → [P]
320ms  a₄ (&#34;-venue&#34;)   + embed([P])      = z₄                → [P]
400ms  a₅ (&#34; incr-&#34;)   + embed([P])      = z₅                → [P]
480ms  a₆ (&#34;-eased&#34;)   + embed([P])      = z₆                → [W] (boundary)
560ms  a₇ (...)        + embed([W])      = z₇                → &#34;The&#34;
640ms  a₈ (...)        + embed(&#34;The&#34;)    = z₈                → [W]
720ms  a₉ (...)        + embed([W])      = z₉                → &#34; revenue&#34;
800ms  a₁₀ (...)       + embed(&#34;revenue&#34;)= z₁₀               → [W]
880ms  a₁₁ (...)       + embed([W])      = z₁₁               → &#34; increased&#34;
...
</code></pre><p><code>[P]</code> = &ldquo;nothing to say yet.&rdquo; <code>[W]</code> = &ldquo;word boundary.&rdquo; Both are filtered from final output.</p>
<p><strong>Why addition, not concatenation?</strong> With concatenation, the dimension doubles (6144) and every layer must distinguish which half is audio vs. text. With addition, dimension stays at 3072 and the network learns to disentangle both signals from the combined representation. More parameter-efficient, no architectural changes needed in the decoder.</p>
<h4 id="language-decoder-with-ada-rms-norm">Language Decoder with Ada RMS-Norm</h4>
<p>The decoder is initialized from <strong>Ministral 3B</strong>, a general-purpose language model — not a narrow transcription specialist.</p>
<p>The decoder uses <strong>GQA</strong> with 32 query heads and 8 KV heads (4:1 ratio), SwiGLU FFN (3072 → 9216 → 3072), and a sliding window of 8192 tokens for bounded-memory decoding on arbitrarily long sequences.</p>
<p><strong>Ada RMS-Norm</strong> allows a single model to operate at any delay from 80ms to 2400ms. Given target delay <code>τ</code>:</p>
<pre tabindex="0"><code>1. Sinusoidal time embedding:
   t_embed = sinusoidal_encoding(τ)          ∈ ℝ^3072

2. Per-layer MLP (inner dim = 32, tiny):
   g_l(τ) = Linear₂(GELU(Linear₁(t_embed))) ∈ ℝ^3072

3. Applied to the FFN branch only:
   r_attn = Attention(RMSNorm(h))            ← NOT conditioned on delay
   h&#39; = h + r_attn
   r_ffn = FFN(RMSNorm(h&#39;) ⊙ (1 + g_l(τ)))  ← conditioned on delay
   y = h&#39; + r_ffn
</code></pre><p>The <code>⊙</code> is element-wise multiplication. When <code>g_l(τ) = 0</code>, this is standard RMSNorm. Non-zero values selectively amplify or suppress dimensions based on delay. Smaller delay → more aggressive early hypotheses; larger delay → wait for more acoustic evidence.</p>
<p>The paper reports this adds about ~5M parameters total. <code>g_l(τ)</code> is computed once per inference session and reused.</p>
<h3 id="64-pipeline-summary-1-second-of-audio">6.4 Pipeline Summary (1 second of audio)</h3>
<pre tabindex="0"><code>16,000 samples
  → mel:         [128, 100]     (128 bins × 100 frames, 100 Hz)
  → after convs: [1280, 50]    (50 Hz)
  → after enc:   [50, 1280]    (32 transformer layers)
  → after adapt: [12, 3072]    (12.5 Hz, 3072-dim)
  → 12 generation steps, one per frame
  → ~3-8 text tokens + [P]/[W] control tokens
</code></pre><h3 id="65-runtime-implications">6.5 Runtime Implications</h3>
<p>Voxtral&rsquo;s architecture is natively streaming. A practical integration:</p>
<ul>
<li>feeds incremental audio frames,</li>
<li>advances one decode step per 80ms audio frame,</li>
<li>emits partial text continuously,</li>
<li>finalizes according to endpointing/segmentation policy.</li>
</ul>
<h3 id="66-generation-loop-shape">6.6 Generation Loop Shape</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Simplified pseudocode for realtime streaming</span>
</span></span><span class="line"><span class="cl"><span class="n">session</span> <span class="o">=</span> <span class="n">start_stream</span><span class="p">(</span><span class="n">delay_ms</span><span class="o">=</span><span class="mi">480</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">incoming_audio_chunks</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">session</span><span class="o">.</span><span class="n">feed_audio</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="n">session</span><span class="o">.</span><span class="n">has_next_frame_step</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">token</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># one decode step per 80ms frame</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="n">PAD_TOKEN</span><span class="p">,</span> <span class="n">WORD_BOUNDARY_TOKEN</span><span class="p">}:</span>
</span></span><span class="line"><span class="cl">            <span class="k">yield</span> <span class="n">token</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">session</span><span class="o">.</span><span class="n">end_stream</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="n">PAD_TOKEN</span><span class="p">,</span> <span class="n">WORD_BOUNDARY_TOKEN</span><span class="p">}:</span>
</span></span><span class="line"><span class="cl">        <span class="k">yield</span> <span class="n">token</span>
</span></span></code></pre></div><p>The loop is <strong>timeline-locked</strong>: exactly one generation step per 80ms audio frame. This is structurally different from unconstrained &ldquo;generate until EOS&rdquo; loops.</p>
<hr>
<h2 id="7-side-by-side-comparison">7. Side-by-Side Comparison</h2>
<h3 id="71-runtime-flow">7.1 Runtime Flow</h3>
<pre tabindex="0"><code class="language-mermaid" data-lang="mermaid">sequenceDiagram
    participant Mic
    participant Q as Qwen-style Pipeline
    participant V as Voxtral-style Pipeline

    Mic-&gt;&gt;Q: audio segment (buffered)
    Q-&gt;&gt;Q: encode full segment
    Q-&gt;&gt;Q: decode tokens to EOS
    Q--&gt;&gt;Mic: transcript chunk

    Mic-&gt;&gt;V: audio chunk 1
    V-&gt;&gt;V: frame-aligned decode steps
    V--&gt;&gt;Mic: partial tokens
    Mic-&gt;&gt;V: audio chunk 2
    V-&gt;&gt;V: continue frame-aligned steps
    V--&gt;&gt;Mic: more partial/final tokens
</code></pre><h3 id="72-tracing-the-revenue-increased-through-both">7.2 Tracing &ldquo;The revenue increased&rdquo; Through Both</h3>
<p><strong>Qwen3-ASR (segment-style):</strong></p>
<pre tabindex="0"><code>1. Capture: accumulate audio in ring buffer until silence/trigger
2. Frontend: mel [128 × 200] from 2 seconds of audio
3. Encode: conv → 18 transformer layers → project → [~25, 1024]
4. Fuse: build prompt, replace 25 pad slots with audio embeddings
5. Decode: generate autoregressively until EOS

   Step 1 → &#34;The&#34;
   Step 2 → &#34; revenue&#34;
   Step 3 → &#34; increased&#34;
   Step 4 → &#34;.&#34;
   Step 5 → EOS

   Total latency = (time to accumulate segment) + encode + decode
</code></pre><p><strong>Voxtral Realtime (streaming, 480ms delay):</strong></p>
<pre tabindex="0"><code>1. Session open with τ = 6 frames (480ms)
2. Feed audio continuously
3. Per-frame decode:

   t=0ms    a₀ + embed(BOS)       → [P]
   t=80ms   a₁ + embed([P])       → [P]
   t=160ms  a₂ + embed([P])       → [P]
   t=240ms  a₃ + embed([P])       → [P]
   t=320ms  a₄ + embed([P])       → [P]
   t=400ms  a₅ + embed([P])       → [P]
   t=480ms  a₆ + embed([P])       → [W]      (first word ready)
   t=560ms  a₇ + embed([W])       → &#34;The&#34;    ← text appears
   t=640ms  a₈ + embed(&#34;The&#34;)     → [W]
   t=720ms  a₉ + embed([W])       → &#34; revenue&#34;
   ...

   Text arrives incrementally, ~480ms behind the speaker.
   No need to wait for silence.
</code></pre><h3 id="73-consolidated-comparison-table">7.3 Consolidated Comparison Table</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Qwen3-ASR-0.6B</th>
          <th>Voxtral-Mini-4B-Realtime-2602</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Parameters</strong></td>
          <td>0.6B</td>
          <td>4.4B</td>
      </tr>
      <tr>
          <td><strong>Mel features</strong></td>
          <td>128 bins, Whisper frontend</td>
          <td>128 bins, Slaney-normalized</td>
      </tr>
      <tr>
          <td><strong>STFT</strong></td>
          <td>n_fft=400, hop=160</td>
          <td>window=400, hop=160²</td>
      </tr>
      <tr>
          <td><strong>Conv downsampling</strong></td>
          <td>3× Conv2D stride 2 (8× total)</td>
          <td>2× CausalConv1D (2×) + adapter 4× (8× total)</td>
      </tr>
      <tr>
          <td><strong>Audio encoder</strong></td>
          <td>18L, d=896, 14 heads, FFN 3584</td>
          <td>32L, d=1280, 32 heads, FFN 5120</td>
      </tr>
      <tr>
          <td><strong>Encoder attention</strong></td>
          <td>Bidirectional, block-chunked</td>
          <td>Causal, sliding window 750</td>
      </tr>
      <tr>
          <td><strong>Encoder norm</strong></td>
          <td>LayerNorm</td>
          <td>RMSNorm</td>
      </tr>
      <tr>
          <td><strong>Encoder position</strong></td>
          <td>Sinusoidal (fixed)</td>
          <td>RoPE (rotary)</td>
      </tr>
      <tr>
          <td><strong>Encoder GQA</strong></td>
          <td>Full MHA (14/14)</td>
          <td>Full MHA (32/32)</td>
      </tr>
      <tr>
          <td><strong>Audio → decoder dim</strong></td>
          <td>1024</td>
          <td>3072</td>
      </tr>
      <tr>
          <td><strong>Fusion method</strong></td>
          <td>Placeholder replacement</td>
          <td>Additive per timestep</td>
      </tr>
      <tr>
          <td><strong>Decoder</strong></td>
          <td>28L, d=1024</td>
          <td>26L, d=3072</td>
      </tr>
      <tr>
          <td><strong>Decoder heads / KV</strong></td>
          <td>16 / 8 (GQA 2:1)</td>
          <td>32 / 8 (GQA 4:1)</td>
      </tr>
      <tr>
          <td><strong>Decoder FFN</strong></td>
          <td>3072</td>
          <td>9216</td>
      </tr>
      <tr>
          <td><strong>Decoder context</strong></td>
          <td>Full sequence</td>
          <td>Sliding window 8192</td>
      </tr>
      <tr>
          <td><strong>Delay conditioning</strong></td>
          <td>None</td>
          <td>Ada RMS-Norm (dim 32, paper-reported ~5M params)</td>
      </tr>
      <tr>
          <td><strong>Vocabulary</strong></td>
          <td>151,936 (Qwen3)</td>
          <td>131,072 (Tekken)</td>
      </tr>
      <tr>
          <td><strong>Streaming</strong></td>
          <td>Official stack supports online/offline; many app integrations remain segment-style</td>
          <td>Native realtime</td>
      </tr>
      <tr>
          <td><strong>Generation loop</strong></td>
          <td>Until EOS (unconstrained)</td>
          <td>One token per 80ms frame (timeline-locked)</td>
      </tr>
      <tr>
          <td><strong>Decoder origin</strong></td>
          <td>ASR-specific</td>
          <td>Ministral 3B (general LLM)</td>
      </tr>
      <tr>
          <td><strong>Quantized size</strong></td>
          <td>~0.6 GB (8-bit)</td>
          <td>~3.1 GB (4-bit)</td>
      </tr>
  </tbody>
</table>
<p>² <code>n_fft</code> not specified in <code>params.json</code>. Implementations may vary.</p>
<hr>
<h2 id="8-the-decoder-as-language-model">8. The Decoder as Language Model</h2>
<h3 id="81-what-this-means">8.1 What This Means</h3>
<p>Qwen3-ASR&rsquo;s decoder is a small (0.6B) ASR-focused decoder. In practice, it is typically used for transcription rather than reliable instruction-style rewriting/summarization, so post-transcription cleanup is usually delegated to a separate LLM.</p>
<p>Voxtral&rsquo;s decoder is Ministral 3B: a general-purpose language model that was taught to understand audio. This is a fundamentally different capability profile.</p>
<h3 id="82-can-voxtrals-decoder-rewrite-text-after-transcription">8.2 Can Voxtral&rsquo;s Decoder Rewrite Text After Transcription?</h3>
<p>Once the audio stream ends, the model could theoretically continue generating in text-only mode. With no audio signal, additive fusion becomes <code>0 + text_embed = text_embed</code> — effectively a standard LM forward pass.</p>
<p>This is <strong>untested</strong>. The model was trained with audio always present. Whether fine-tuning preserved or destroyed Ministral 3B&rsquo;s text-only instruction-following is an empirical question that should be validated before building architecture around it.</p>
<p>A practical policy:</p>
<ol>
<li>Attempt backend-native rewrite only if capability is validated.</li>
<li>Otherwise fall back to a dedicated text LLM post-processor.</li>
</ol>
<h3 id="83-can-you-swap-the-decoder-for-a-different-language-model">8.3 Can You Swap the Decoder for a Different Language Model?</h3>
<p>The paper&rsquo;s training recipe is designed to support this:</p>
<ol>
<li>
<p><strong>Phase 1 (5% of training)</strong>: Freeze decoder. Train only encoder + adapter. LR: 4×10⁻⁴. This lets the randomly initialized encoder produce useful embeddings without destabilizing the pre-trained decoder.</p>
</li>
<li>
<p><strong>Phase 2 (95% of training)</strong>: Unfreeze everything. Train end-to-end. LR: 6×10⁻⁵, batch size ~370 hours, AdamW with z-loss regularization.</p>
</li>
</ol>
<p>To swap the decoder:</p>
<ol>
<li>Pick an LM with compatible architecture. If hidden dim ≠ 3072, retrain the adapter.</li>
<li>Add Ada RMS-Norm conditioning (~5M extra params).</li>
<li>Follow the same two-phase recipe with speech + word-level timestamps in target language(s).</li>
</ol>
<p>The bottleneck is <strong>data</strong>: the DSM framework requires knowing exactly when each word was spoken to construct training targets. Large-scale word-level timestamp annotations are expensive.</p>
<hr>
<h2 id="9-abstraction-design-implications">9. Abstraction Design Implications</h2>
<h3 id="91-what-can-be-unified">9.1 What Can Be Unified</h3>
<ul>
<li>Input audio format contract (16 kHz mono float32).</li>
<li>Output text event contract (partial/final tokens, segment boundaries).</li>
<li>Session lifecycle and error handling.</li>
<li>Metrics and tracing hooks.</li>
</ul>
<h3 id="92-what-should-remain-backend-specific">9.2 What Should Remain Backend-Specific</h3>
<ul>
<li>Feature extractor internals (Whisper vs. Slaney mel).</li>
<li>Tokenizer internals and special token semantics.</li>
<li>Generation loop mechanics (unconstrained vs. timeline-locked).</li>
<li>Delay / endpointing control policy.</li>
</ul>
<h3 id="93-failure-modes-to-avoid">9.3 Failure Modes to Avoid</h3>
<p><strong>Over-abstraction</strong>: if the abstraction hides critical timing semantics, you get incorrect latency assumptions, brittle endpointing, and misleading capability signals.</p>
<p><strong>Under-abstraction</strong>: if every backend leaks all internals upward, you get duplicated pipeline logic, inconsistent event semantics, and harder testing.</p>
<h3 id="94-practical-interface-shape">9.4 Practical Interface Shape</h3>
<p>A capability-driven session interface:</p>
<ul>
<li><code>start_session(...)</code></li>
<li><code>feed_audio(...)</code></li>
<li><code>poll_events(...)</code></li>
<li><code>end_session(...)</code></li>
</ul>
<p>with explicit capability flags:</p>
<ul>
<li><code>native_streaming: bool</code></li>
<li><code>delay_control: bool</code></li>
<li><code>word_timestamps: bool</code></li>
<li><code>rewrite_experimental: bool</code></li>
</ul>
<p>This preserves model differences without duplicating pipeline/UI logic.</p>
<hr>
<h2 id="10-runtime-considerations">10. Runtime Considerations</h2>
<h3 id="101-throughput-vs-decode-schedule">10.1 Throughput vs Decode Schedule</h3>
<p>Two distinct constraints:</p>
<ol>
<li><strong>Segment-style</strong>: total decode time must be less than segment wall time.</li>
<li><strong>Timeline-locked</strong>: each decode step must complete within its 80ms frame budget.</li>
</ol>
<p>Similar in spirit, different in failure mode.</p>
<h3 id="102-endpointing">10.2 Endpointing</h3>
<p>Even with native realtime models, turn segmentation remains an application-level choice:</p>
<ul>
<li>VAD-driven endpointing,</li>
<li>model-token-driven endpointing (sequences of [P] tokens as silence signal),</li>
<li>or hybrid endpointing.</li>
</ul>
<h3 id="103-memory">10.3 Memory</h3>
<p>Realtime note-taking products may co-host:</p>
<ul>
<li>ASR backend (~0.6 GB or ~3.1 GB),</li>
<li>optional intent model,</li>
<li>optional rewrite LLM (~3-4 GB).</li>
</ul>
<p>If Voxtral&rsquo;s decoder can handle rewriting, total memory drops significantly. If not, Voxtral (3.1 GB) + external LLM (3-4 GB) may strain 16 GB machines.</p>
<h3 id="104-partial-transcript-stability">10.4 Partial Transcript Stability</h3>
<p>Aggressive low-delay settings increase early revisions; conservative delay reduces revisions but increases perceived lag. This tradeoff is orthogonal to raw WER and should be evaluated separately.</p>
<hr>
<h2 id="11-source-validation-notes">11. Source Validation Notes</h2>
<h3 id="111-common-misreports-for-qwen3-asr-06b">11.1 Common Misreports for Qwen3-ASR-0.6B</h3>
<p>Values frequently wrong in derivative writeups:</p>
<table>
  <thead>
      <tr>
          <th>Field</th>
          <th>Often stated</th>
          <th>Actual (from config.json)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Decoder hidden size</td>
          <td>2048</td>
          <td><strong>1024</strong></td>
      </tr>
      <tr>
          <td>Audio encoder dim</td>
          <td>1024</td>
          <td><strong>896</strong></td>
      </tr>
      <tr>
          <td>Audio encoder layers</td>
          <td>24</td>
          <td><strong>18</strong></td>
      </tr>
      <tr>
          <td>Audio encoder FFN</td>
          <td>4096</td>
          <td><strong>3584</strong></td>
      </tr>
      <tr>
          <td>Audio output projection</td>
          <td>2048</td>
          <td><strong>1024</strong></td>
      </tr>
  </tbody>
</table>
<p>These mismatches arise because many discussions blend paper-level reference architectures, larger Qwen3 variants, and checkpoint-specific configs. For deployment, the checkpoint config is the source of truth.</p>
<h3 id="112-voxtral-variant-confusion">11.2 Voxtral Variant Confusion</h3>
<p>The earlier <strong>Voxtral-Mini-3B-2507</strong> (non-realtime) has a different architecture:</p>
<ul>
<li>Bidirectional encoder (not causal)</li>
<li>Cross-attention fusion (not additive)</li>
<li>Different decoder dimensions</li>
</ul>
<p>Values from the non-realtime variant should not be mixed with the Realtime-2602 checkpoint. Always verify against the specific checkpoint&rsquo;s <code>params.json</code>.</p>
<h3 id="113-validation-checklist">11.3 Validation Checklist</h3>
<p>When documenting ASR checkpoints, prefer:</p>
<ol>
<li><code>config.json</code> / <code>params.json</code> over third-party summaries.</li>
<li>Explicitly verify: layer counts, hidden/FFN dims, head counts, sliding windows, vocab size, special token IDs.</li>
<li>Cite both paper and checkpoint config; treat checkpoint config as definitive.</li>
<li>Pin commit hashes for strict reproducibility.</li>
</ol>
<hr>
<h2 id="12-references">12. References</h2>
<p>Primary sources used for numbers and claims:</p>
<ol>
<li>Qwen3-ASR-0.6B model card: <a href="https://huggingface.co/Qwen/Qwen3-ASR-0.6B">huggingface.co/Qwen/Qwen3-ASR-0.6B</a></li>
<li>Qwen3-ASR-0.6B config.json: <a href="https://huggingface.co/Qwen/Qwen3-ASR-0.6B/blob/main/config.json">huggingface.co/Qwen/Qwen3-ASR-0.6B/blob/main/config.json</a></li>
<li>Qwen3-ASR-0.6B preprocessor_config.json: <a href="https://huggingface.co/Qwen/Qwen3-ASR-0.6B/blob/main/preprocessor_config.json">huggingface.co/Qwen/Qwen3-ASR-0.6B/blob/main/preprocessor_config.json</a></li>
<li>Qwen3-ASR technical report: <a href="https://arxiv.org/abs/2601.21337">arxiv.org/abs/2601.21337</a></li>
<li>Voxtral-Mini-4B-Realtime-2602 model card: <a href="https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602">huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602</a></li>
<li>Voxtral-Mini-4B-Realtime-2602 params.json: <a href="https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602/blob/main/params.json">huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602/blob/main/params.json</a></li>
<li>Voxtral Realtime paper (DSM architecture): <a href="https://arxiv.org/abs/2602.11298">arxiv.org/abs/2602.11298</a></li>
<li>MLX-community Voxtral 4-bit weights: <a href="https://huggingface.co/mlx-community/Voxtral-Mini-4B-Realtime-2602-4bit">huggingface.co/mlx-community/Voxtral-Mini-4B-Realtime-2602-4bit</a></li>
</ol>
<hr>
<p><em>Written while building <a href="https://github.com/jaju/voissistant">voissistant</a>, a real-time speech-to-text system for Apple Silicon using MLX.</em></p>

            </div>
        </article></main>
</div>

<footer class="footer">
    <small class="footer_copyright">
        © 2026 Ravindra R. Jaju.
        Powered by <a href="https://github.com/hugo-sid/hugo-blog-awesome" target="_blank" rel="noopener">Hugo blog awesome</a>.
    </small>
</footer>


<div class="fixed-social-icons">
    <a href="https://github.com/jaju" target="_blank" rel="noopener noreferrer me"
       title="GitHub" class="social-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
    </a>
    <a href="https://x.com/jaju" target="_blank" rel="noopener noreferrer me"
       title="X (Twitter)" class="social-icon">
        <svg viewBox="0 0 1200 1227" fill="currentColor" xmlns="http://www.w3.org/2000/svg">
    <path
        d="M714.163 519.284L1160.89 0H1055.03L667.137 450.887L357.328 0H0L468.492 681.821L0 1226.37H105.866L515.491 750.218L842.672 1226.37H1200L714.137 519.284H714.163ZM569.165 687.828L521.697 619.934L144.011 79.6944H306.615L611.412 515.685L658.88 583.579L1055.08 1150.3H892.476L569.165 687.854V687.828Z"/>
</svg>
    </a>
</div>





    
    
        
    



    
    <script async src="http://localhost:1313/js/main.js" ></script>

    
        
        <script async src="http://localhost:1313/js/custom.js" ></script>
    

</body>
</html>
