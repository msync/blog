<!doctype html>
<html class="no-js" lang="en-us">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<title>LLMs: Exploring To Understand | Sync&#39;ing from Memory</title>
<meta property="og:title" content="LLMs: Exploring To Understand"/>

<meta property="og:description" content="Delving into LLMs to probe how they do what they do."/>

<meta name="twitter:card" content="summary"/>
<meta name="twitter:site" content="@jaju"/>
<meta name="twitter:creator" content="@jaju"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>


<link rel="stylesheet" type="text/css" href="http://localhost:1313/css/bundle.css">
<link rel="stylesheet" type="text/css" href="http://localhost:1313/css/hljs/rainbow.css">

</head>

<body>

<div id="body">
    <header id="site-header">
        <div class="content">
  <a href="http://localhost:1313/">Sync&#39;ing from Memory</a>
  <nav>
    
    <a href="http://localhost:1313/notes/">Notes</a>
    
    <a href="http://localhost:1313/posts/">Posts</a>
    
  </nav>
</div>
    </header>

    <div id="main-wrapper">
        
<div class="content notes">

    <div class="toc">
        <nav id="TableOfContents">
  <ol>
    <li>
      <ol>
        <li><a href="#a-token-of-appreciation">A Token of Appreciation</a></li>
        <li><a href="#what-is-a-language-model">What is a Language Model?</a></li>
        <li><a href="#modelve-model-delve">MoDelve: Model Delve</a>
          <ol>
            <li><a href="#the-sizing-dot-dot-dot">The sizing&hellip;</a></li>
            <li><a href="#inspecting-our-model">Inspecting our model</a></li>
          </ol>
        </li>
        <li><a href="#code">Code</a>
          <ol>
            <li><a href="#files">Files</a></li>
          </ol>
        </li>
      </ol>
    </li>
  </ol>
</nav>
    </div>

    <article itemscope itemtype="http://schema.org/CreativeWork">

        <header>
            <h1 itemprop="title">LLMs: Exploring To Understand</h1>
            <summary>
                Delving into LLMs to probe how they do what they do.
            </summary>
            <div class="meta">
                
                <div class="time">
                <time datetime="2024-04-29 18:29:59 &#43;0530 IST" itemprop="datePublished" class="pubdate">
                    <span class="weekday">Mon</span>,
                    <span class="day">29</span>
                    <span class="month">Apr</span>,
                    <span class="year">2024</span>
                </time>

                
                <time datetime="2024-06-23 18:37:01 &#43;0530 IST" itemprop="dateModified" class="pubdate">
                    Updated:
                    <span class="weekday">Sun</span>,
                    <span class="day">23</span>
                    <span class="month">Jun</span>,
                    <span class="year">2024</span>
                </time>
                
                </div>
            </div>
        </header>

        <main>
            <h2 id="a-token-of-appreciation">A Token of Appreciation</h2>
<blockquote>
<p>Delve. Delve. <a href="https://www.reddit.com/r/ChatGPT/comments/1bzv071/apparently_the_word_delve_is_the_biggest/">Delve</a>!
For what are we, our selve,
If not the mind,
That likes to delve.</p>
</blockquote>
<p>I wish you deeply <em>embed</em> in your memory, a <em>token</em> of appreciation for the previous <a href="https://msync.org/notes/llm-understanding-tokens-embeddings/">article</a> in this series.</p>
<p>Indeed, I have a lame sense of humor. But all I mean to say is, there will be references to the previous article linked above, and it might be helpful to keep that in <em>context</em>.</p>
<h2 id="what-is-a-language-model">What is a Language Model?</h2>
<p>Please don&rsquo;t run away. This is a customary section. Every article needs to explain what an LM means, while ensuring that the reader ends up being further bored, and no wiser. Like, er, <a href="https://www.reddit.com/r/ProgrammerHumor/comments/wj2yms/okay_so_have_you_heard_about_monads/">monads</a>!</p>
<p>Play time. Let&rsquo;s play autocomplete.</p>
<div class="sample">
<p>The ball hit the bat, and <span class="underline">_</span></p>
</div>
<p>What did you think of? Cricket?</p>
<p>For most of the <a href="https://en.wikipedia.org/wiki/Member_states_of_the_Commonwealth_of_Nations">member nations</a> of the <a href="https://en.wikipedia.org/wiki/Commonwealth_of_Nations">Commonwealth</a> (my country being one), cricket is a sport first. If the British wouldn&rsquo;t have arrived, but English would have, it&rsquo;d have been the <a href="https://en.wikipedia.org/wiki/Cricket_(insect)">insect</a> first.</p>
<p>So, here are a few possible completions that might not surprise you much if you were reminded of the sport.</p>
<ul>
<li><em>The ball hit the bat, and</em> <span class="underline">it raced to the boundary.</span></li>
<li><em>The ball hit the bat, and</em> <span class="underline">off it flew into the fielder&rsquo;s hand!</span></li>
<li><em>The ball hit the bat, and</em> <span class="underline">the players took off for the winning run.</span></li>
</ul>
<p>Here&rsquo;s a completely different one.</p>
<div class="sample">
<p><em>The ball hit the bat, and</em> it fell down to the ground, gravely injured.</p>
</div>
<p>Depending on your current frame of mind, it might take you just a wee bit more time to regroup your thinking and anchor the word <strong>bat</strong> into the animal kingdom context, and then it all makes sense.</p>
<p>I&rsquo;m not going to belabor this point much - but the term <span class="underline">language model</span>, in the context of the term <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a>, is used to describe a set of characteristics of a language captured in a neural network&rsquo;s architecture and values of the weights and biases stored there-in.</p>
<p>So, if you were continuing your discussion with a biologist, the above sentence would veer towards the animal kingdom. Otherwise, towards the sport. But, it would depend on the <strong>context</strong>. We&rsquo;ll keep it to the sport context, and here&rsquo;s one last before we move on&hellip;</p>
<blockquote>
<p>The ball hit the bat, and the bat swung hard, pushing it away furiously.
Why, of course. That bat was being handled by the one and only <a href="https://en.wikipedia.org/wiki/Sachin_Tendulkar">Sachin.</a></p>
</blockquote>
<h2 id="modelve-model-delve">MoDelve: Model Delve</h2>
<p>Let us get started by exploring a reasonably sized, and high quality LLM (as of the first half of 2024) - the <a href="https://huggingface.co/microsoft/Phi-3-mini-128k-instruct">microsoft/Phi-3-mini-128k-instruct</a>.</p>
<h3 id="the-sizing-dot-dot-dot">The sizing&hellip;</h3>
<p>This is the most important engineering aspect we should get out of the way before moving to our primary track.</p>
<p>As we deal with large, billion-scale parameter models, and code to go by the side, we have to be mindful of the kind of memory and processing that will be demanded.</p>
<p>Let&rsquo;s choose a model that is small, but high quality. And, as I explain below, we need to make it smaller still for a regular developer machine with about <strong>16 gigs</strong> of RAM. (If you are on a lower-configuration machine in 2024, I feel sorry. I already feel sorry for 16&hellip;)</p>
<p>The <a href="https://huggingface.co/microsoft/Phi-3-mini-128k-instruct">microsoft/Phi-3-mini-128k-instruct</a> model can <a href="https://arxiv.org/abs/2404.14219">run on a phone</a>. But there are a few things we must understand clearly before we move forward.</p>
<p>The model size is indicated in terms of the number of parameters it holds - which are all the <span class="underline">weights</span> and <span class="underline">biases</span> of the network. Each of these values is a numeric value held in specific data-types that have their own sizes.</p>
<p>Na√Øvely speaking, if each parameter is held in a 32-bit float data-type, then we get the total size in bytes by multiplying the number of parameters by <strong>4</strong>. A so-called <strong>4B</strong> model will take up roughly <strong>16 GB</strong> of memory.</p>
<p>We can do away with the need for 32 bits for our numbers, because, it turns out, reducing the fidelity of these numbers to 16, 8 and 4 bits does not <strong>always</strong> bring down the quality of the inference in a perceptible manner. Oh, let me emphasize this - we are only talking about downsizing during inference time. Training computations are sensitive and we will keep that out of scope for this article.</p>
<p>This idea of down-sizing parameters is called <strong>quantization</strong>, and it is a pretty useful idea that allows us to run inferencing on less expensive hardware, and even low-power devices. Be aware that quantization is not about downsizing every single number you come across in a model&rsquo;s representation, and certainly not downsizing in the same manner <span class="underline">across the board</span>.</p>
<p>We&rsquo;ll implicitly use quantization in our examples that follow, using the <a href="https://github.com/huggingface/optimum-quanto">quanto</a> backend (you can read an overview <a href="https://huggingface.co/blog/quanto-introduction">here</a>), or whatever that makes sense. That way, I avoid running into memory requirement problems.</p>
<h3 id="inspecting-our-model">Inspecting our model</h3>
<p>Let&rsquo;s get going. It always gives us a kick to go on a test drive before we buy something - like this new model. First, the looks!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoModelForCausalLM, QuantoConfig
</span></span><span style="display:flex;"><span>model_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;microsoft/Phi-3-mini-128k-instruct&#34;</span>
</span></span><span style="display:flex;"><span>device <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;cpu&#34;</span> <span style="color:#75715e"># Can be &#34;mps&#34; on Apple Silicon, or &#34;cuda&#34; if you have it.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>quantization_config <span style="color:#f92672">=</span> QuantoConfig(weights<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;int4&#34;</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModelForCausalLM<span style="color:#f92672">.</span>from_pretrained(model_name,
</span></span><span style="display:flex;"><span>                                             quantization_config<span style="color:#f92672">=</span>quantization_config,
</span></span><span style="display:flex;"><span>                                             device_map<span style="color:#f92672">=</span>device,
</span></span><span style="display:flex;"><span>                                             trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(model)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>`flash-attention` package not found, consider installing for better performance: No module named &#39;flash_attn&#39;.
</span></span><span style="display:flex;"><span>Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation=&#39;eager&#39;`.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Loading checkpoint shards:   0% 0/2 [00:00&lt;?, ?it/s]
</span></span><span style="display:flex;"><span>Loading checkpoint shards:  50% 1/2 [00:06&lt;00:06,  6.97s/it]
</span></span><span style="display:flex;"><span>Loading checkpoint shards: 100% 2/2 [00:11&lt;00:00,  5.79s/it]
</span></span><span style="display:flex;"><span>Loading checkpoint shards: 100% 2/2 [00:11&lt;00:00,  5.97s/it]
</span></span><span style="display:flex;"><span>Phi3ForCausalLM(
</span></span><span style="display:flex;"><span>  (model): Phi3Model(
</span></span><span style="display:flex;"><span>    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)
</span></span><span style="display:flex;"><span>    (embed_dropout): Dropout(p=0.0, inplace=False)
</span></span><span style="display:flex;"><span>    (layers): ModuleList(
</span></span><span style="display:flex;"><span>      (0-31): 32 x Phi3DecoderLayer(
</span></span><span style="display:flex;"><span>        (self_attn): Phi3Attention(
</span></span><span style="display:flex;"><span>          (o_proj): QLinear(in_features=3072, out_features=3072, bias=False)
</span></span><span style="display:flex;"><span>          (qkv_proj): QLinear(in_features=3072, out_features=9216, bias=False)
</span></span><span style="display:flex;"><span>          (rotary_emb): Phi3SuScaledRotaryEmbedding()
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (mlp): Phi3MLP(
</span></span><span style="display:flex;"><span>          (gate_up_proj): QLinear(in_features=3072, out_features=16384, bias=False)
</span></span><span style="display:flex;"><span>          (down_proj): QLinear(in_features=8192, out_features=3072, bias=False)
</span></span><span style="display:flex;"><span>          (activation_fn): SiLU()
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (input_layernorm): Phi3RMSNorm()
</span></span><span style="display:flex;"><span>        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
</span></span><span style="display:flex;"><span>        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
</span></span><span style="display:flex;"><span>        (post_attention_layernorm): Phi3RMSNorm()
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    (norm): Phi3RMSNorm()
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><aside>
<p>The notice about using &ldquo;flash-attention&rdquo; can be ignored without worrying about missing on any concepts. On the Mac, I haven&rsquo;t found a way to use it without effort. It&rsquo;s an optimization to improve the efficiency of training and inference by reducing memory copies. A brief explanation can be found <a href="https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention">here</a>.</p>
</aside>
<p>Note the use of <code>AutoModelForCausalLM</code>. It knows how to pick the right instance from a list of <a href="https://github.com/huggingface/transformers/blob/9ba9369a2557e53a01378199a9839ec6e82d8bc7/src/transformers/models/auto/modeling_auto.py#L430">supported</a> causal LMs. (Note: The link to the list is to a specific git commit and the latest version may be different.)</p>
<p>Note the layers in the model.</p>
<ul>
<li>A few layers (embedding, dropout) in the beginning. The embedding layer reads input of size <span class="underline">32064</span> and converts them to the embedding dimension of 3072</li>
<li>32 <code>Phi3Attention</code> layers</li>
<li>1 <code>Phi3MLP</code></li>
<li>A few more - <code>Phi3RMSNorm</code>, dropouts</li>
<li>Finally, a <code>Linear</code> layer that transforms a 3072-dimensional vector to a 32064-dimensional output.</li>
</ul>
<p>32064 is the size of the vocabulary - the distinct number of tokens the model uses to convert the input, or generate the output. If you inspect the model data, the number of tokens is 32011 (32000 + 11 special tokens). 32064 seems to be what the <a href="https://github.com/huggingface/transformers">transformers</a> library has chosen as the token vocabulary size, and I suspect the tokens 32011 onwards are unused (0-based counter).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(model_name)
</span></span><span style="display:flex;"><span>print(tokenizer)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</span></span><span style="display:flex;"><span>LlamaTokenizerFast(name_or_path=&#39;microsoft/Phi-3-mini-128k-instruct&#39;, vocab_size=32000, model_max_length=131072, is_fast=True, padding_side=&#39;left&#39;, truncation_side=&#39;right&#39;, special_tokens={&#39;bos_token&#39;: &#39;&lt;s&gt;&#39;, &#39;eos_token&#39;: &#39;&lt;|endoftext|&gt;&#39;, &#39;unk_token&#39;: &#39;&lt;unk&gt;&#39;, &#39;pad_token&#39;: &#39;&lt;|endoftext|&gt;&#39;}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
</span></span><span style="display:flex;"><span>	0: AddedToken(&#34;&lt;unk&gt;&#34;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
</span></span><span style="display:flex;"><span>	1: AddedToken(&#34;&lt;s&gt;&#34;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
</span></span><span style="display:flex;"><span>	2: AddedToken(&#34;&lt;/s&gt;&#34;, rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),
</span></span><span style="display:flex;"><span>	32000: AddedToken(&#34;&lt;|endoftext|&gt;&#34;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
</span></span><span style="display:flex;"><span>	32001: AddedToken(&#34;&lt;|assistant|&gt;&#34;, rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),
</span></span><span style="display:flex;"><span>	32002: AddedToken(&#34;&lt;|placeholder1|&gt;&#34;, rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),
</span></span><span style="display:flex;"><span>	32003: AddedToken(&#34;&lt;|placeholder2|&gt;&#34;, rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),
</span></span><span style="display:flex;"><span>	32004: AddedToken(&#34;&lt;|placeholder3|&gt;&#34;, rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),
</span></span><span style="display:flex;"><span>	32005: AddedToken(&#34;&lt;|placeholder4|&gt;&#34;, rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),
</span></span><span style="display:flex;"><span>	32006: AddedToken(&#34;&lt;|system|&gt;&#34;, rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),
</span></span><span style="display:flex;"><span>	32007: AddedToken(&#34;&lt;|end|&gt;&#34;, rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),
</span></span><span style="display:flex;"><span>	32008: AddedToken(&#34;&lt;|placeholder5|&gt;&#34;, rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),
</span></span><span style="display:flex;"><span>	32009: AddedToken(&#34;&lt;|placeholder6|&gt;&#34;, rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),
</span></span><span style="display:flex;"><span>	32010: AddedToken(&#34;&lt;|user|&gt;&#34;, rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Let&rsquo;s explore some specific ones to understand. Nothing gets printed for 32011, and is likely empty as conjectured.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(<span style="color:#ae81ff">0</span>, tokenizer<span style="color:#f92672">.</span>decode([<span style="color:#ae81ff">0</span>]))
</span></span><span style="display:flex;"><span>print(<span style="color:#ae81ff">1</span>, tokenizer<span style="color:#f92672">.</span>decode([<span style="color:#ae81ff">1</span>]))
</span></span><span style="display:flex;"><span>print(<span style="color:#ae81ff">1001</span>, tokenizer<span style="color:#f92672">.</span>decode([<span style="color:#ae81ff">1001</span>]))
</span></span><span style="display:flex;"><span>print(<span style="color:#ae81ff">15987</span>, tokenizer<span style="color:#f92672">.</span>decode([<span style="color:#ae81ff">15987</span>]))
</span></span><span style="display:flex;"><span>print(<span style="color:#ae81ff">32009</span>, tokenizer<span style="color:#f92672">.</span>decode([<span style="color:#ae81ff">32009</span>]))
</span></span><span style="display:flex;"><span>print(<span style="color:#ae81ff">32010</span>, tokenizer<span style="color:#f92672">.</span>decode([<span style="color:#ae81ff">32010</span>]))
</span></span><span style="display:flex;"><span>print(<span style="color:#ae81ff">32011</span>, tokenizer<span style="color:#f92672">.</span>decode([<span style="color:#ae81ff">32011</span>]))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>0 &lt;unk&gt;
</span></span><span style="display:flex;"><span>1 &lt;s&gt;
</span></span><span style="display:flex;"><span>1001 ER
</span></span><span style="display:flex;"><span>15987 effect
</span></span><span style="display:flex;"><span>32009 &lt;|placeholder6|&gt;
</span></span><span style="display:flex;"><span>32010 &lt;|user|&gt;
</span></span><span style="display:flex;"><span>32011
</span></span></code></pre></div><p><strong>FIXME</strong> This is a direct pick from the <a href="https://github.com/huggingface/transformers">transformers</a> library, with a few minor tweaks that allows us to delve into some finer parts.</p>
<p>Let&rsquo;s see what the model can do for a <span class="underline">Hello World</span>. Depending on your hardware, this can take some non-trivial amount of time. Note that we have used a quantized model - using 4-bit representations for the parameters. So, while memory should not be a problem, the number of computations is very high. The CPU is going to be busy. If you have a GPU supported by torch, you might be lucky to experience some good speedup.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>example_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Today is&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The .to(device) is not needed and defaults to using the &#34;cpu&#34;</span>
</span></span><span style="display:flex;"><span>example_text_tokens <span style="color:#f92672">=</span> tokenizer([example_text], return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>example_text_generated_ids <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>generate(<span style="color:#f92672">**</span>example_text_tokens)
</span></span><span style="display:flex;"><span>example_text_generated <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>batch_decode(example_text_generated_ids, skip_special_tokens<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>print(example_text_generated[<span style="color:#ae81ff">0</span>])
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Today is a great day to be alive.
</span></span></code></pre></div><p>Let&rsquo;s convert this into a function a try a few more examples</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">complete_the_input_text</span>(prompt):
</span></span><span style="display:flex;"><span>    prompt_tokens <span style="color:#f92672">=</span> tokenizer([prompt], return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>    generated_ids <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>generate(<span style="color:#f92672">**</span>prompt_tokens)
</span></span><span style="display:flex;"><span>    generated_text <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>batch_decode(generated_ids, skip_special_tokens<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> generated_text[<span style="color:#ae81ff">0</span>]
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(complete_the_input_text(<span style="color:#e6db74">&#34;India is a&#34;</span>))
</span></span><span style="display:flex;"><span>print(complete_the_input_text(<span style="color:#e6db74">&#34;Pakistan is a&#34;</span>))
</span></span><span style="display:flex;"><span>print(complete_the_input_text(<span style="color:#e6db74">&#34;China is a&#34;</span>))
</span></span><span style="display:flex;"><span>print(complete_the_input_text(<span style="color:#e6db74">&#34;Burma is a&#34;</span>))
</span></span><span style="display:flex;"><span>print(complete_the_input_text(<span style="color:#e6db74">&#34;June 21 is a&#34;</span>))
</span></span><span style="display:flex;"><span>print(complete_the_input_text(<span style="color:#e6db74">&#34;Oracle is a&#34;</span>))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>&#39;India is a country in South Asia. It is located in the Indian subcontinent and is&#39;
</span></span><span style="display:flex;"><span>&#39;Pakistan is a country that has been facing a significant challenge in terms of its population growth. The&#39;
</span></span><span style="display:flex;"><span>&#39;China is a country with a large population and a growing economy, but also a country with a&#39;
</span></span><span style="display:flex;"><span>&#39;Burma is a country in Southeast Asia. It is located in the region known as&#39;
</span></span><span style="display:flex;"><span>&#39;June 21 is a Wednesday&#39;
</span></span><span style="display:flex;"><span>&#39;Oracle is a database management system that is part of the Oracle Corporation suite of database applications. It&#39;
</span></span></code></pre></div><p>**</p>
<h2 id="code">Code</h2>
<h3 id="files">Files</h3>
<ul>
<li><a href="llm_explore.py">llm_explore.py</a></li>
</ul>

        </main>

    </article>

</div>

    </div>

    <footer id="site-footer">
        <div class="hide-for-small-only float-left author">
    <span>&copy; <span
            class="author-name">Ravindra R. Jaju</span> &mdash; 2015-2024</span>
</div>

<div class="social-media-list float-right">
  <span class="social-media-item">
    <a href="https://github.com/jaju">
      <span class="svg-social-icon icon--github">
        <svg viewBox="0 0 16 16">
          <path fill="#828282"
                d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
        </svg>
      </span>
      <span class="username">jaju</span>
    </a>
  </span>
  <span class="social-media-item">
    <a href="https://twitter.com/jaju">
      <span class="svg-social-icon icon--twitter">
        <svg viewBox="0 0 16 16">
          <path fill="#828282"
                d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809 c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
        </svg>
      </span>
      <span class="username">jaju</span>
    </a>
  </span>
</div>

    </footer>
</div>

<script src="/js/bundle.js"></script>
<script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-TYE506V9R8"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-TYE506V9R8');
</script>
</body>
</html>
