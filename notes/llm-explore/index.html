<!doctype html>
<html class="no-js" lang="en-us">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<title>LLMs: Exploring To Understand | Sync&#39;ing from Memory</title>
<meta property="og:title" content="LLMs: Exploring To Understand"/>

<meta property="og:description" content="Delving into LLMs to probe how they do what they do."/>

<meta name="twitter:card" content="summary"/>
<meta name="twitter:site" content="@jaju"/>
<meta name="twitter:creator" content="@jaju"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>


<link rel="stylesheet" type="text/css" href="http://localhost:1313/css/bundle.css">
<link rel="stylesheet" type="text/css" href="http://localhost:1313/css/hljs/rainbow.css">

</head>

<body>

<div id="body">
    <header id="site-header">
        <div class="content">
  <a href="http://localhost:1313/">Sync&#39;ing from Memory</a>
  <nav>
    
    <a href="http://localhost:1313/notes/">Notes</a>
    
    <a href="http://localhost:1313/posts/">Posts</a>
    
  </nav>
</div>
    </header>

    <div id="main-wrapper">
        
<div class="content notes">

    <div class="toc">
        <nav id="TableOfContents">
  <ol>
    <li>
      <ol>
        <li><a href="#a-token-of-appreciation">A Token of Appreciation</a>
          <ol>
            <li><a href="#what-are-we-talking-about-here">What are we talking about here?</a></li>
          </ol>
        </li>
        <li><a href="#quick-dot-what-is-a-language-model">Quick. What is a Language Model?</a></li>
        <li><a href="#testing-the-waters">Testing the waters</a>
          <ol>
            <li><a href="#first-the-sizing-dot-dot-dot">First, the sizing&hellip;</a></li>
            <li><a href="#inspecting-our-model">Inspecting our model</a></li>
          </ol>
        </li>
        <li><a href="#code">Code</a>
          <ol>
            <li><a href="#code">Code</a></li>
          </ol>
        </li>
      </ol>
    </li>
  </ol>
</nav>
    </div>

    <article itemscope itemtype="http://schema.org/CreativeWork">

        <header>
            <h1 itemprop="title">LLMs: Exploring To Understand</h1>
            <summary>
                Delving into LLMs to probe how they do what they do.
            </summary>
            <div class="meta">
                
                <div class="time">
                <time datetime="2024-04-29 18:29:59 &#43;0530 IST" itemprop="datePublished" class="pubdate">
                    <span class="weekday">Mon</span>,
                    <span class="day">29</span>
                    <span class="month">Apr</span>,
                    <span class="year">2024</span>
                </time>

                
                <time datetime="2024-05-07 18:36:52 &#43;0530 IST" itemprop="dateModified" class="pubdate">
                    Updated:
                    <span class="weekday">Tue</span>,
                    <span class="day">7</span>
                    <span class="month">May</span>,
                    <span class="year">2024</span>
                </time>
                
                </div>
            </div>
        </header>

        <main>
            <h2 id="a-token-of-appreciation">A Token of Appreciation</h2>
<blockquote>
<p>Delve. Delve. <a href="https://www.reddit.com/r/ChatGPT/comments/1bzv071/apparently_the_word_delve_is_the_biggest/">Delve</a>!
For what are we, our selve,
If not the mind,
That likes to delve.</p>
</blockquote>
<p>I wish you deeply <em>embed</em> in your memory, a <em>token</em> of appreciation for the previous <a href="https://msync.org/notes/llm-understanding-tokens-embeddings/">article</a> in this series.</p>
<p>Indeed, I have a lame sense of humor. But all I mean to say is, there will be references to the previous article linked above, and it might be helpful to keep that in <em>context</em>.</p>
<h3 id="what-are-we-talking-about-here">What are we talking about here?</h3>
<p>Code, and just enough references to theory to explain the code.</p>
<h2 id="quick-dot-what-is-a-language-model">Quick. What is a Language Model?</h2>
<div class="sample">
<p>The ball hit the bat, and <span class="underline">_</span></p>
</div>
<p>How would you complete the above sentence?</p>
<p>For most of the <a href="https://en.wikipedia.org/wiki/Member_states_of_the_Commonwealth_of_Nations">member nations</a> of the <a href="https://en.wikipedia.org/wiki/Commonwealth_of_Nations">Commonwealth</a> (my country being one), cricket is a sport first. If the British wouldn&rsquo;t have arrived, but English would have, it&rsquo;d have been the <a href="https://en.wikipedia.org/wiki/Cricket_(insect)">insect</a> first.</p>
<p>So, here are a few possible completions that might not surprise you much if you were reminded of the sport.</p>
<ul>
<li><em>The ball hit the bat, and</em> <span class="underline">it raced to the boundary.</span></li>
<li><em>The ball hit the bat, and</em> <span class="underline">off it flew into the fielder&rsquo;s hand!</span></li>
<li><em>The ball hit the bat, and</em> <span class="underline">the players took off for the winning run.</span></li>
</ul>
<p>Here&rsquo;s another completion</p>
<div class="sample">
<p><em>The ball hit the bat, and</em> it fell down to the ground, gravely injured.</p>
</div>
<p>Depending on your current frame of mind, it might take you just a wee bit more time to regroup your thinking and anchor the word <strong>bat</strong> into the animal kingdom context, and then it all makes sense.</p>
<p>I&rsquo;m not going to belabor this point much - but the term <span class="underline">language model</span>, in the context of the term <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a>, is used to describe a set of characteristics of a language captured in a neural network&rsquo;s architecture and values of the weights and biases stored there-in.</p>
<p>Artificial neural networks are based on our understanding of how animal brains work, and it is definitely a simplification and currently, much smaller and inefficient too as compared to the biological brains.</p>
<p>Better LLMs are loosely analogous to one or all of higher IQ, more memory, and fast thinking. We can debate and discuss this elsewhere, but we now move on to the exploration.</p>
<blockquote>
<p>The ball hit the bat, and the bat swung hard, pushing it away furiously.
Why, of course. It was being handled by the one and only <a href="https://en.wikipedia.org/wiki/Sachin_Tendulkar">Sachin.</a></p>
</blockquote>
<h2 id="testing-the-waters">Testing the waters</h2>
<h3 id="first-the-sizing-dot-dot-dot">First, the sizing&hellip;</h3>
<p><a href="https://huggingface.co/microsoft/Phi-3-mini-128k-instruct">microsoft/Phi-3-mini-128k-instruct</a> is a small but powerful model that we will use for reference. It can in fact <a href="https://arxiv.org/abs/2404.14219">run on a phone</a>. But there are a few things we must understand clearly before we move forward.</p>
<p>The model size is indicated in terms of the number of parameters it holds - which are all the so-called <span class="underline">weights</span> and <span class="underline">biases</span> of the network. But furthermore, each of these values is a numeric value held in data-types that have their own sizes.</p>
<p>Na√Øvely speaking, if each parameter is held in a 32-bit float data-type, then we get the total size in bytes by multiplying the number of parameters by <strong>4</strong>. A so-called <strong>8B</strong> model will take up <strong>32 GB</strong> of memory.</p>
<p>In reality, there are various techniques in use that attempt to reduce the memory requirements while keeping the computations safely within some bounds so that the model behaves well. During training, the computation of the updates is extremely sensitive towards the precision of the values of the parameters. But during inference, the precision matters to a relatively much lesser degree. And thus, various optimizations like representing parameters in low-fidelity data-types - like 16, 8, 4 bits - have been found to reduce memory requirements considerably while keeping accuracy and quality within acceptable bounds.</p>
<p>The act of down-sizing parameters is called <strong>quantization</strong>, and it is a very useful technique that allows us to run inferencing on less expensive hardware, and even low-power devices.</p>
<p>We&rsquo;ll use quantization in our example that follows. For me, it keeps my laptop responsive while not forcing me to kill other applications like my browser with some 100 tabs. Oh, well&hellip;</p>
<h3 id="inspecting-our-model">Inspecting our model</h3>
<p>We load the model as follows. Note the use of <code>AutoModelForCausalLM</code> instead of <code>AutoModel</code>. Don&rsquo;t read much into it - very likely just a software choice quirk in the version being used where <code>AutoModel</code> isn&rsquo;t able to instantiate the right instance.</p>
<aside>
<p>The notice about using &ldquo;flash-attention&rdquo; can be ignored without worrying about missing on any concepts. It&rsquo;s an optimization to improve the efficiency of training and inference by reducing memory copies. A brief explanation can be found <a href="https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention">here</a>.</p>
</aside>
<p>It always gives us a kick to go on a test drive before we buy something - like this new model. Let&rsquo;s see what all it does. First, some generation. This is a direct pick from the <a href="https://github.com/huggingface/transformers">transformers</a> library, with a few minor tweaks that allows us to delve into some finer parts.</p>
<h2 id="code">Code</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer, AutoModelForCausalLM, QuantoConfig
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;microsoft/Phi-3-mini-128k-instruct&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(model_name)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>quantization_config <span style="color:#f92672">=</span> QuantoConfig(weights<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;int8&#34;</span>)
</span></span><span style="display:flex;"><span>quantized_model <span style="color:#f92672">=</span> AutoModelForCausalLM<span style="color:#f92672">.</span>from_pretrained(model_name,
</span></span><span style="display:flex;"><span>                                                       quantization_config<span style="color:#f92672">=</span>quantization_config,
</span></span><span style="display:flex;"><span>                                                       device_map<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;cpu&#34;</span>,
</span></span><span style="display:flex;"><span>                                                       trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(quantized_model)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer([<span style="color:#e6db74">&#34;A list of colors: red, blue&#34;</span>], return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>)
</span></span><span style="display:flex;"><span>generated_ids <span style="color:#f92672">=</span> quantized_model<span style="color:#f92672">.</span>generate(<span style="color:#f92672">**</span>model_inputs)
</span></span><span style="display:flex;"><span>print(generated_ids)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>generated_text <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>batch_decode(generated_ids, skip_special_tokens<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>print(generated_text)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</span></span><span style="display:flex;"><span>`flash-attention` package not found, consider installing for better performance: No module named &#39;flash_attn&#39;.
</span></span><span style="display:flex;"><span>Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation=&#39;eager&#39;`.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Loading checkpoint shards:   0% 0/2 [00:00&lt;?, ?it/s]
</span></span><span style="display:flex;"><span>Loading checkpoint shards:  50% 1/2 [00:05&lt;00:05,  5.97s/it]
</span></span><span style="display:flex;"><span>Loading checkpoint shards: 100% 2/2 [00:10&lt;00:00,  5.30s/it]
</span></span><span style="display:flex;"><span>Loading checkpoint shards: 100% 2/2 [00:10&lt;00:00,  5.40s/it]
</span></span><span style="display:flex;"><span>Phi3ForCausalLM(
</span></span><span style="display:flex;"><span>  (model): Phi3Model(
</span></span><span style="display:flex;"><span>    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)
</span></span><span style="display:flex;"><span>    (embed_dropout): Dropout(p=0.0, inplace=False)
</span></span><span style="display:flex;"><span>    (layers): ModuleList(
</span></span><span style="display:flex;"><span>      (0-31): 32 x Phi3DecoderLayer(
</span></span><span style="display:flex;"><span>        (self_attn): Phi3Attention(
</span></span><span style="display:flex;"><span>          (o_proj): QLinear(in_features=3072, out_features=3072, bias=False)
</span></span><span style="display:flex;"><span>          (qkv_proj): QLinear(in_features=3072, out_features=9216, bias=False)
</span></span><span style="display:flex;"><span>          (rotary_emb): Phi3SuScaledRotaryEmbedding()
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (mlp): Phi3MLP(
</span></span><span style="display:flex;"><span>          (gate_up_proj): QLinear(in_features=3072, out_features=16384, bias=False)
</span></span><span style="display:flex;"><span>          (down_proj): QLinear(in_features=8192, out_features=3072, bias=False)
</span></span><span style="display:flex;"><span>          (activation_fn): SiLU()
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (input_layernorm): Phi3RMSNorm()
</span></span><span style="display:flex;"><span>        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
</span></span><span style="display:flex;"><span>        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
</span></span><span style="display:flex;"><span>        (post_attention_layernorm): Phi3RMSNorm()
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    (norm): Phi3RMSNorm()
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>/Users/jaju/github/org-notes/llm-explore/.venv/v/lib/python3.12/site-packages/transformers/generation/utils.py:1152: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
</span></span><span style="display:flex;"><span>  warnings.warn(
</span></span><span style="display:flex;"><span>You are not running the flash-attention implementation, expect numerical differences.
</span></span><span style="display:flex;"><span>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
</span></span><span style="display:flex;"><span>To disable this warning, you can either:
</span></span><span style="display:flex;"><span>	- Avoid using `tokenizers` before the fork if possible
</span></span><span style="display:flex;"><span>	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
</span></span><span style="display:flex;"><span>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
</span></span><span style="display:flex;"><span>To disable this warning, you can either:
</span></span><span style="display:flex;"><span>	- Avoid using `tokenizers` before the fork if possible
</span></span><span style="display:flex;"><span>	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
</span></span><span style="display:flex;"><span>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
</span></span><span style="display:flex;"><span>To disable this warning, you can either:
</span></span><span style="display:flex;"><span>	- Avoid using `tokenizers` before the fork if possible
</span></span><span style="display:flex;"><span>	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
</span></span><span style="display:flex;"><span>tensor([[    1,   319,  1051,   310, 11955, 29901,  2654, 29892,  7254, 29892,
</span></span><span style="display:flex;"><span>          7933, 29892, 13328, 29892,  3708,   552, 29892, 24841, 29892,  4628]])
</span></span><span style="display:flex;"><span>[&#39;A list of colors: red, blue, green, yellow, purple, orange, black&#39;]
</span></span></code></pre></div><h3 id="code">Code</h3>
<ul>
<li><a href="llm_explore.py">llm_explore.py</a></li>
</ul>

        </main>

    </article>

</div>

    </div>

    <footer id="site-footer">
        <div class="hide-for-small-only float-left author">
    <span>&copy; <span
            class="author-name">Ravindra R. Jaju</span> &mdash; 2015-2024</span>
</div>

<div class="social-media-list float-right">
  <span class="social-media-item">
    <a href="https://github.com/jaju">
      <span class="svg-social-icon icon--github">
        <svg viewBox="0 0 16 16">
          <path fill="#828282"
                d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
        </svg>
      </span>
      <span class="username">jaju</span>
    </a>
  </span>
  <span class="social-media-item">
    <a href="https://twitter.com/jaju">
      <span class="svg-social-icon icon--twitter">
        <svg viewBox="0 0 16 16">
          <path fill="#828282"
                d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809 c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
        </svg>
      </span>
      <span class="username">jaju</span>
    </a>
  </span>
</div>

    </footer>
</div>

<script src="/js/bundle.js"></script>
<script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-TYE506V9R8"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-TYE506V9R8');
</script>
</body>
</html>
