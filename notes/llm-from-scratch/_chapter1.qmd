# Chapter 1: Understanding Large Language Models {#sec-chapter-1}

- A good quality corpus is crucial.
- Choice of corpus will depend on the downstream tasks.
- For example, to generate code, the corpus should contain a substantial amount of code snippets.

An open-source corpus of 3 trillion tokens has been detailed in [@dolma2024].

## Key challenges in pre-training
  - Data: Creating or curating a large corpus of text data is a significant challenge. The quality and diversity of the data are crucial for the performance of the model.
  - Model: The model architecture is key for pre-training. A good model architecture will help in learning the underlying structure of the data.
  - Budget: Pre-training a large language model requires a lot of computational resources. A good budget is crucial for pre-training.
  - [GPT-3](https://en.wikipedia.org/wiki/GPT-3) potentially cost $4.6 million to train, per @gpt3technicaloverview.

@instruct-gpt presented ideas on how to fine-tune GPT-3 on a dataset of instructions.
