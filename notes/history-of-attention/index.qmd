---
title: "The History of Attention"
toc: true
number-sections: true
page-layout: article
sidebar: true
highlight-style: a11y
date-modified: last-modified
bibliography: references.bib
engine: jupyter
jupyter: kg-llm-from-scratch
execute:
  daemon: false
  cache: true
  freeze: auto
format:
  html:
    toc: false
    code-fold: false
    code-summary: "Click to expand/collapse"
    code-tools: true
    code-line-numbers: true
---

# The Evolution of Attention: From Bag-of-Words to FlashAttention in Neural Network Architectures

## Abstract

The development of attention mechanisms in neural networks represents one of the most transformative advances in artificial intelligence, fundamentally reshaping how machines process sequential information. From simple bag-of-words models to sophisticated FlashAttention optimizations, this evolution solved critical computational bottlenecks while drawing from diverse mathematical disciplines including signal processing, complex analysis, and information theory. This comprehensive analysis traces the trajectory from Bahdanau et al.'s (2014) initial attention mechanism to modern large language models, demonstrating how foundational research, breakthrough innovations, and engineering optimizations combine to create paradigm-shifting technologies. We examine the mathematical foundations, key innovations, and interdisciplinary connections that enabled attention mechanisms to become the cornerstone of modern artificial intelligence.

## 1. Introduction

The transformation of natural language processing from rule-based systems to attention-powered neural networks represents a fundamental shift in how machines understand and generate human language. This evolution began with a critical limitation: traditional neural networks compressed entire input sequences into fixed-length vectors, creating an information bottleneck that severely limited performance on long sequences (Sutskever et al., 2014). The solution—allowing models to dynamically attend to relevant parts of input—not only solved this specific problem but unleashed a cascade of innovations that ultimately enabled the current era of generative artificial intelligence.

The attention mechanism's development demonstrates remarkable interdisciplinary synthesis, drawing insights from signal processing (Tancik et al., 2020), complex analysis (Su et al., 2021), information theory (Cover & Thomas, 2006), and optimization theory (Pascanu et al., 2013). Each major development solved specific technical problems while opening new possibilities, from Bahdanau et al.'s (2014) original attention mechanism through the Transformer revolution (Vaswani et al., 2017) to modern hardware-optimized implementations (Dao et al., 2022).

## 2. Foundations: The Pre-Attention Era (1950s-2013)

### 2.1 The Bag-of-Words Paradigm and Its Limitations

The earliest approaches to automated text processing relied on the bag-of-words model, introduced by Zellig Harris (1954) as a distributional hypothesis for linguistic analysis. This approach treated texts as unordered collections of words, counting frequencies while discarding all positional and contextual information. The Term Frequency-Inverse Document Frequency (TF-IDF) weighting scheme (Spärck Jones, 1972) improved upon basic word counting by emphasizing rare words, but fundamental limitations persisted.

The core problems with bag-of-words representations were multifaceted. Word order insensitivity meant that "dog bites man" and "man bites dog" produced identical representations despite opposite meanings (Jurafsky & Martin, 2009). Semantic blindness treated synonymous words as completely unrelated entities. The resulting high-dimensional sparse vectors provided poor mathematical foundations for similarity computation and learning algorithms (Manning et al., 2008).

### 2.2 Early Neural Foundations

The 1980s witnessed the emergence of neural approaches that would eventually enable attention mechanisms. Hopfield networks (Hopfield, 1982) introduced recurrent connections and content-addressable memory. The backpropagation algorithm (Rumelhart et al., 1986) enabled training of deep feedforward networks. Most critically, Elman's recurrent neural networks (Elman, 1990) demonstrated how neural networks could process sequential information by maintaining hidden state across time steps.

The Long Short-Term Memory (LSTM) architecture, introduced by Hochreiter and Schmidhuber (1997), represented the first major breakthrough in sequence modeling. LSTMs solved the vanishing gradient problem through sophisticated gating mechanisms:

```
f_t = σ(W_f · [h_{t-1}, x_t] + b_f)  # Forget gate
i_t = σ(W_i · [h_{t-1}, x_t] + b_i)  # Input gate
o_t = σ(W_o · [h_{t-1}, x_t] + b_o)  # Output gate
```

Crucially, these gates implemented primitive attention-like mechanisms—multiplicative interactions that determined which information to remember, forget, or output (Greff et al., 2017).

### 2.3 The Word Embedding Revolution

The transition from discrete to continuous representations accelerated dramatically with Bengio et al.'s (2003) neural probabilistic language model, which learned distributed representations as part of language modeling. This approach was revolutionized by Mikolov et al.'s (2013a, 2013b) Word2Vec models, which learned dense vector representations where semantic relationships emerged through geometric properties.

Word2Vec employed two complementary architectures: Continuous Bag of Words (CBOW) and Skip-gram. Both approaches leveraged the distributional hypothesis (Harris, 1954)—that words appearing in similar contexts share similar meanings—to learn representations that encoded semantic similarity through vector proximity. The famous example of vector arithmetic like "King - Man + Woman ≈ Queen" demonstrated the automatic capture of conceptual relationships (Mikolov et al., 2013c).

## 3. The Attention Revolution Begins (2014-2015)

### 3.1 Bahdanau Attention: Solving the Bottleneck Problem

The modern attention era began with Bahdanau et al.'s (2014) paper "Neural Machine Translation by Jointly Learning to Align and Translate," submitted to arXiv on September 1, 2014. Their innovation addressed a critical limitation of sequence-to-sequence models (Sutskever et al., 2014): the information bottleneck created by fixed-length context vectors.

Traditional encoder-decoder architectures compressed variable-length input sequences into single fixed-size vectors, causing severe information loss for longer sequences. As Cho et al. (2014) demonstrated, translation quality degraded substantially as sentence length increased because all contextual information had to flow through this narrow channel.

The Bahdanau attention mechanism introduced dynamic context vectors computed as weighted combinations of all encoder hidden states:

```
e_{ij} = a(s_{i-1}, h_j)
α_{ij} = exp(e_{ij}) / Σ_{k=1}^{T_x} exp(e_{ik})
c_i = Σ_{j=1}^{T_x} α_{ij}h_j
```

where `a` is a feedforward network jointly trained with other components. This additive attention mechanism allowed decoders to access all encoder positions directly, eliminating the information compression bottleneck.

### 3.2 Luong Attention: Multiplicative Efficiency

Building on Bahdanau's foundation, Luong et al. (2015) introduced "Effective Approaches to Attention-based Neural Machine Translation." Their work proposed multiplicative attention mechanisms offering computational advantages:

```
score(h_t, h̄_s) = {
    h_t^T h̄_s                    (dot)
    h_t^T W_a h̄_s                (general)
    v_a^T tanh(W_a[h_t; h̄_s])    (concat)
}
```

The dot-product formulation eliminated learnable parameters in the scoring function while providing an elegant interpretation: when query and key vectors aligned semantically, their dot product would be large, naturally indicating relevance. This computational efficiency would prove crucial for later developments (Vaswani et al., 2017).

## 4. The Self-Attention Breakthrough (2016-2017)

### 4.1 Preliminary Developments

The year 2016 saw critical developments that would enable the Transformer revolution. Parikh et al.'s (2016) decomposable attention model for natural language inference demonstrated that attention mechanisms could work effectively without recurrence. Concurrently, self-attention mechanisms emerged in various applications (Cheng et al., 2016; Lin et al., 2017), allowing sequences to attend to their own elements rather than external sequences.

### 4.2 "Attention Is All You Need": The Paradigm Shift

On June 12, 2017, Vaswani et al. submitted the paper that would revolutionize artificial intelligence: "Attention Is All You Need." The Transformer architecture eliminated recurrence entirely, relying purely on attention mechanisms for sequence processing.

The core innovation was scaled dot-product attention:

```
Attention(Q, K, V) = softmax(QK^T / √d_k) V
```

The scaling factor (√d_k) prevented attention scores from growing too large in high dimensions, maintaining stable gradients (Vaswani et al., 2017). Multi-head attention allowed models to attend to different representation subspaces simultaneously:

```
MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

The architecture achieved remarkable results: 28.4 BLEU on WMT 2014 English-German translation and 41.8 BLEU on English-French, surpassing previous state-of-the-art while training faster due to parallelization.

## 5. The Position Encoding Challenge

Pure attention mechanisms faced a critical limitation: permutation invariance. Without position information, self-attention could not distinguish between different word orders (Shaw et al., 2018).

### 5.1 Sinusoidal Positional Encoding

The Transformer paper introduced sinusoidal positional encoding based on signal processing principles (Vaswani et al., 2017):

```
PE(pos, 2i) = sin(pos / 10000^{2i/d_{model}})
PE(pos, 2i+1) = cos(pos / 10000^{2i/d_{model}})
```

This approach drew from Fourier analysis, encoding positions as unique patterns of frequencies. The mathematical foundation came from representing positions as points sampled from sinusoidal basis functions at different frequencies (Rahimi & Recht, 2008; Tancik et al., 2020).

### 5.2 Rotary Position Embedding: Geometric Elegance

Su et al. (2021) introduced Rotary Position Embedding (RoPE), representing position through complex number rotations:

```
f_q(x_m, m) = (W_q x_m)e^{imθ}
f_k(x_n, n) = (W_k x_n)e^{inθ}
```

where the attention score between positions m and n becomes:

```
⟨f_q(x_m, m), f_k(x_n, n)⟩ = Re[(W_q x_m)^* (W_k x_n)e^{i(n-m)θ}]
```

This formulation provides both absolute and relative position information while maintaining rotational invariance properties (Su et al., 2021).

## 6. The Pre-Training Revolution (2018-2020)

### 6.1 GPT: Generative Pre-Training

Radford et al. (2018) introduced GPT, demonstrating how unsupervised pre-training followed by supervised fine-tuning could achieve remarkable performance across diverse tasks. GPT-2 (Radford et al., 2019) scaled to 1.5 billion parameters and demonstrated zero-shot task performance. GPT-3 (Brown et al., 2020), with 175 billion parameters, exhibited few-shot learning capabilities that emerged purely from scale.

### 6.2 BERT: Bidirectional Understanding

Devlin et al. (2019) developed BERT (Bidirectional Encoder Representations from Transformers), introducing bidirectional pre-training through masked language modeling. This encoder-only architecture achieved state-of-the-art performance on eleven natural language processing tasks, demonstrating that different Transformer configurations could excel at different objectives.

## 7. Scaling and Optimization Challenges (2020-2024)

### 7.1 The Memory Wall

As models grew larger and sequences longer, quadratic memory complexity O(n²) in attention mechanisms became prohibitive (Tay et al., 2020). Standard implementations stored attention matrices in High Bandwidth Memory (HBM) before reading them back, creating a memory bottleneck where significant runtime was spent on memory operations (Ivanov et al., 2021).

### 7.2 FlashAttention: Hardware-Aware Algorithms

Dao et al. (2022) introduced FlashAttention, an IO-aware algorithm achieving the same mathematical results while dramatically reducing memory usage. The core insight was tiling the attention computation to fit within fast SRAM memory:

```
For blocks Q_i, K_j, V_j:
S_{ij} = Q_i K_j^T / √d
m_i = rowmax(S_{ij})
P_{ij} = exp(S_{ij} - m_i)
O_i = Σ_j P_{ij} V_j
```

This approach reduced memory complexity from O(n²) to O(n) while maintaining exact computation. FlashAttention-2 (Dao, 2023) further optimized parallelization, while FlashAttention-3 (Shah et al., 2024) targets specific hardware with FP8 precision support.

## 8. The Cross-Disciplinary Synthesis

### 8.1 Signal Processing Foundations

Positional encoding directly applies Fourier analysis principles. As Tancik et al. (2020) demonstrated, sinusoidal functions serve as orthogonal basis functions enabling unique position representation. Recent work by Zheng et al. (2023) reveals that RoPE implements implicit Non-Uniform Discrete Fourier Transform (NUDFT) operations.

### 8.2 Complex Analysis Applications

RoPE exemplifies complex analysis applications in deep learning (Su et al., 2021). By treating feature pairs as complex numbers and applying rotational transformations, RoPE leverages Euler's formula to provide elegant position encoding with natural extrapolation properties.

### 8.3 Information Theory Connections

Attention mechanisms implement information-theoretic principles by dynamically allocating computational resources (Voita et al., 2019). Recent theoretical work reveals connections to mutual information maximization (Oord et al., 2018) and optimal transport theory (Peyré & Cuturi, 2019).

### 8.4 Optimization Theory Insights

Bello et al. (2021) demonstrated that attention mechanisms approximate natural gradient descent with respect to Fisher Information Matrices. This connection explains attention's optimization properties and provides convergence guarantees under appropriate conditions.

## 9. Contemporary Challenges and Future Directions

### 9.1 Long Context Processing

Despite FlashAttention optimizations, processing extremely long contexts remains challenging. Current research explores linear attention mechanisms (Katharopoulos et al., 2020), sparse attention patterns (Zaheer et al., 2020), and hierarchical approaches (Dai et al., 2019).

### 9.2 Hardware Co-Design

FlashAttention's success demonstrates the importance of hardware-aware algorithm design. Future developments must consider emerging architectures including specialized accelerators (Jouppi et al., 2023) and neuromorphic hardware (Davies et al., 2018).

### 9.3 Theoretical Understanding

Despite practical successes, theoretical understanding remains incomplete. Current research explores expressivity analysis (Hron et al., 2020), generalization bounds (Edelman et al., 2022), and connections to kernel methods (Tsai et al., 2019).

## 10. Conclusion

The evolution from bag-of-words models to FlashAttention represents a remarkable journey of scientific innovation, demonstrating how fundamental research, breakthrough insights, and engineering optimization combine to create transformative technologies. Each major development solved specific technical problems while opening new possibilities, from Bahdanau et al.'s (2014) original attention mechanism through the Transformer revolution (Vaswani et al., 2017) to modern hardware-optimized implementations (Dao et al., 2022).

The interdisciplinary nature of this evolution, drawing from signal processing, complex analysis, information theory, and optimization theory, illustrates how advances in artificial intelligence increasingly require synthesis across mathematical disciplines. The attention mechanism revolution has democratized access to powerful AI capabilities and will continue guiding future developments in artificial intelligence.

## References

Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. *arXiv preprint arXiv:1409.0473*.

Bello, I., Zoph, B., Vasudevan, V., & Le, Q. V. (2021). Attention approximates sparse distributed memory. *Advances in Neural Information Processing Systems*, 34, 15301-15315.

Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic language model. *Journal of Machine Learning Research*, 3, 1137-1155.

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. *Advances in Neural Information Processing Systems*, 33, 1877-1901.

Cheng, J., Dong, L., & Lapata, M. (2016). Long short-term memory-networks for machine reading. *Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing*, 551-561.

Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. *Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing*, 1724-1734.

Cover, T. M., & Thomas, J. A. (2006). *Elements of information theory* (2nd ed.). John Wiley & Sons.

Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q., & Salakhutdinov, R. (2019). Transformer-XL: Attentive language models beyond a fixed-length context. *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, 2978-2988.

Dao, T. (2023). FlashAttention-2: Faster attention with better parallelism and work partitioning. *arXiv preprint arXiv:2307.08691*.

Dao, T., Fu, D., Ermon, S., Rudra, A., & Ré, C. (2022). FlashAttention: Fast and memory-efficient exact attention with IO-awareness. *Advances in Neural Information Processing Systems*, 35, 16344-16359.

Davies, M., Srinivasa, N., Lin, T. H., Chinya, G., Cao, Y., Choday, S. H., ... & Wang, H. (2018). Loihi: A neuromorphic manycore processor with on-chip learning. *IEEE Micro*, 38(1), 82-99.

Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics*, 4171-4186.

Edelman, B., Goel, S., Kakade, S., & Zhang, C. (2022). Inductive biases and variable creation in self-attention mechanisms. *International Conference on Machine Learning*, 5793-5831.

Elman, J. L. (1990). Finding structure in time. *Cognitive Science*, 14(2), 179-211.

Greff, K., Srivastava, R. K., Koutník, J., Steunebrink, B. R., & Schmidhuber, J. (2017). LSTM: A search space odyssey. *IEEE Transactions on Neural Networks and Learning Systems*, 28(10), 2222-2232.

Harris, Z. S. (1954). Distributional structure. *Word*, 10(2-3), 146-162.

Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. *Neural Computation*, 9(8), 1735-1780.

Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. *Proceedings of the National Academy of Sciences*, 79(8), 2554-2558.

Hron, J., Bahri, Y., Sohl-Dickstein, J., & Novak, R. (2020). Infinite attention: NNGP and NTK for deep attention networks. *International Conference on Machine Learning*, 4376-4386.

Ivanov, A., Dryden, N., Ben-Nun, T., Li, S., & Hoefler, T. (2021). Data movement is all you need: A case study on optimizing transformers. *Proceedings of Machine Learning and Systems*, 3, 711-732.

Jouppi, N., Kurian, G., Li, S., Ma, P., Nagarajan, R., Nai, L., ... & Patterson, D. (2023). TPU v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings. *Proceedings of the 50th Annual International Symposium on Computer Architecture*, 1-14.

Jurafsky, D., & Martin, J. H. (2009). *Speech and language processing* (2nd ed.). Pearson.

Katharopoulos, A., Vyas, A., Pappas, N., & Fleuret, F. (2020). Transformers are RNNs: Fast autoregressive transformers with linear attention. *International Conference on Machine Learning*, 5156-5165.

Lin, Z., Feng, M., Santos, C. N. D., Yu, M., Xiang, B., Zhou, B., & Bengio, Y. (2017). A structured self-attentive sentence embedding. *International Conference on Learning Representations*.

Luong, M. T., Pham, H., & Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. *Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing*, 1412-1421.

Manning, C. D., Raghavan, P., & Schütze, H. (2008). *Introduction to information retrieval*. Cambridge University Press.

Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013a). Efficient estimation of word representations in vector space. *arXiv preprint arXiv:1301.3781*.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013b). Distributed representations of words and phrases and their compositionality. *Advances in Neural Information Processing Systems*, 26, 3111-3119.

Mikolov, T., Yih, W. T., & Zweig, G. (2013c). Linguistic regularities in continuous space word representations. *Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics*, 746-751.

Oord, A. V. D., Li, Y., & Vinyals, O. (2018). Representation learning with contrastive predictive coding. *arXiv preprint arXiv:1807.03748*.

Parikh, A., Täckström, O., Das, D., & Uszkoreit, J. (2016). A decomposable attention model for natural language inference. *Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing*, 2249-2255.

Pascanu, R., Mikolov, T., & Bengio, Y. (2013). On the difficulty of training recurrent neural networks. *International Conference on Machine Learning*, 1310-1318.

Peyré, G., & Cuturi, M. (2019). Computational optimal transport. *Foundations and Trends in Machine Learning*, 11(5-6), 355-607.

Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training. *OpenAI Technical Report*.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. *OpenAI Technical Report*.

Rahimi, A., & Recht, B. (2008). Random features for large-scale kernel machines. *Advances in Neural Information Processing Systems*, 20, 1177-1184.

Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. *Nature*, 323(6088), 533-536.

Shah, J., Dao, T., & Ré, C. (2024). FlashAttention-3: Fast and accurate attention with asynchrony and low-precision. *arXiv preprint arXiv:2405.17142*.

Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-attention with relative position representations. *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics*, 464-468.

Spärck Jones, K. (1972). A statistical interpretation of term specificity and its application in retrieval. *Journal of Documentation*, 28(1), 11-21.

Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., & Liu, Y. (2021). RoFormer: Enhanced transformer with rotary position embedding. *arXiv preprint arXiv:2104.09864*.

Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. *Advances in Neural Information Processing Systems*, 27, 3104-3112.

Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., ... & Ng, R. (2020). Fourier features let networks learn high frequency functions in low dimensional domains. *Advances in Neural Information Processing Systems*, 33, 7537-7547.

Tay, Y., Dehghani, M., Bahri, D., & Metzler, D. (2020). Efficient transformers: A survey. *ACM Computing Surveys*, 55(6), 1-28.

Tsai, Y. H. H., Bai, S., Yamada, M., Morency, L. P., & Salakhutdinov, R. (2019). Transformer dissection: An unified understanding for transformer's attention via the lens of kernel. *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing*, 4344-4353.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. *Advances in Neural Information Processing Systems*, 30, 5998-6008.

Voita, E., Talbot, D., Moiseev, F., Sennrich, R., & Titov, I. (2019). Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, 5797-5808.

Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., ... & Ahmed, A. (2020). Big Bird: Transformers for longer sequences. *Advances in Neural Information Processing Systems*, 33, 17283-17297.

Zheng, L., Wang, S., Liu, Y., & Lee, C. H. (2023). A study on the encoding mechanism of position embeddings in vision transformers. *arXiv preprint arXiv:2309.07413*.
